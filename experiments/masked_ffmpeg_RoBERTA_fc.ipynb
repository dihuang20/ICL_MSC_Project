{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# This source code file refers to:\n",
    "# https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n",
    "# https://huggingface.co/docs/transformers/model_doc/roberta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import os\n",
    "import random\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments, RobertaModel\n",
    "\n",
    "# The following randomization refers to: https://github.com/ICL-ml4csec/VulBERTa/blob/main/Finetuning_VulBERTa-MLP.ipynb\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "MODEL_SAVE_PATH = '/root/autodl-tmp/finetuned_models'\n",
    "DATASET_ROOT_PATH = '/root/autodl-tmp'\n",
    "BERT_CONFIG = 'roberta-large' # roberta-base, roberta-large\n",
    "MODEL_NAME = 'fc'\n",
    "labels = {0:0, 1:1}\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "ONLY_TESTING = False\n",
    "\n",
    "DATASET_NAME = 'ffmpeg'\n",
    "DATASET_MASKING = 'masked_'\n",
    "# DATASET_MASKING = ''\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 15\n",
    "LR = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "MODEL_SAVE_PATH: /root/autodl-tmp/finetuned_models/masked_ffmpeg\n",
      "DATASET_PATH: /root/autodl-tmp/output_dataset_1/masked_ffmpeg\n",
      "BERT_CONFIG: roberta-large\n",
      "ONLY_TESTING: False\n",
      "DATASET_NAME: ffmpeg\n",
      "DATASET_MASKING: masked_\n",
      "BATCH_SIZE: 4\n",
      "EPOCHS: 15\n",
      "LR: 1e-06\n",
      "using device: cuda\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(BERT_CONFIG)\n",
    "DATASET_PATH = f'{DATASET_ROOT_PATH}/output_dataset_1/{DATASET_MASKING}{DATASET_NAME}'\n",
    "MODEL_SAVE_PATH = f'{MODEL_SAVE_PATH}/{DATASET_MASKING}{DATASET_NAME}'\n",
    "\n",
    "print('seed:', seed)\n",
    "print('MODEL_SAVE_PATH:', MODEL_SAVE_PATH)\n",
    "print('DATASET_PATH:', DATASET_PATH)\n",
    "print('BERT_CONFIG:', BERT_CONFIG)\n",
    "\n",
    "print('ONLY_TESTING:', ONLY_TESTING)\n",
    "print('DATASET_NAME:', DATASET_NAME)\n",
    "print('DATASET_MASKING:', DATASET_MASKING)\n",
    "\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "print('EPOCHS:', EPOCHS)\n",
    "print('LR:', LR)\n",
    "\n",
    "print('using device:', device)\n",
    "print('GPU count:', torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "mkdir_if_not_exist(MODEL_SAVE_PATH)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['label']]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['commit_message']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = RobertaModel.from_pretrained(BERT_CONFIG)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if BERT_CONFIG == 'roberta-large':\n",
    "            self.linear = nn.Linear(1024, len(labels))\n",
    "        else:\n",
    "            self.linear = nn.Linear(768, len(labels))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        return linear_output\n",
    "\n",
    "    def check_parameters(self):\n",
    "        print('The number of Bert parameters:', self.bert.num_parameters())\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "        bert_config = 'large' if BERT_CONFIG == 'roberta-large' else 'base'\n",
    "        val_acc = f'{total_acc_val / len(val_data):.3f}'\n",
    "        torch.save(model.state_dict(), f'{MODEL_SAVE_PATH}/roberta_{bert_config}_{MODEL_NAME}_{val_acc}_ep{epoch_num + 1}.pt')\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    total_acc_test = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            test_label = test_label.data.cpu().numpy()\n",
    "            predic = output.argmax(dim=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, test_label)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    print(report)\n",
    "    print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 2612/2612 [10:32<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.131             | Train Accuracy:  0.697             | Val Loss:  0.095             | Val Accuracy:  0.771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:34<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.086             | Train Accuracy:  0.796             | Val Loss:  0.086             | Val Accuracy:  0.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.077             | Train Accuracy:  0.808             | Val Loss:  0.089             | Val Accuracy:  0.777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.073             | Train Accuracy:  0.812             | Val Loss:  0.089             | Val Accuracy:  0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.071             | Train Accuracy:  0.813             | Val Loss:  0.093             | Val Accuracy:  0.792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.070             | Train Accuracy:  0.819             | Val Loss:  0.091             | Val Accuracy:  0.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:34<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.071             | Train Accuracy:  0.818             | Val Loss:  0.086             | Val Accuracy:  0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.071             | Train Accuracy:  0.814             | Val Loss:  0.092             | Val Accuracy:  0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.071             | Train Accuracy:  0.815             | Val Loss:  0.087             | Val Accuracy:  0.796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:34<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.070             | Train Accuracy:  0.816             | Val Loss:  0.088             | Val Accuracy:  0.794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.070             | Train Accuracy:  0.820             | Val Loss:  0.086             | Val Accuracy:  0.801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.069             | Train Accuracy:  0.821             | Val Loss:  0.087             | Val Accuracy:  0.798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.070             | Train Accuracy:  0.815             | Val Loss:  0.088             | Val Accuracy:  0.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:33<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.071             | Train Accuracy:  0.810             | Val Loss:  0.088             | Val Accuracy:  0.795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:34<00:00,  4.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.070             | Train Accuracy:  0.818             | Val Loss:  0.091             | Val Accuracy:  0.798\n",
      "Test Accuracy:  0.788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7939    0.8516    0.8218      1995\n",
      "  vulnerable     0.7798    0.7038    0.7399      1489\n",
      "\n",
      "    accuracy                         0.7885      3484\n",
      "   macro avg     0.7868    0.7777    0.7808      3484\n",
      "weighted avg     0.7879    0.7885    0.7868      3484\n",
      "\n",
      "[[1699  296]\n",
      " [ 441 1048]]\n"
     ]
    }
   ],
   "source": [
    "if not ONLY_TESTING:\n",
    "    model = BertClassifier()\n",
    "    model.to(device)\n",
    "    \n",
    "    df_train = pd.read_json(f'{DATASET_PATH}/train.json')\n",
    "    df_val = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "    df_test = pd.read_json(f'{DATASET_PATH}/test.json')\n",
    "    train(model, df_train, df_val, LR, EPOCHS)\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_files_list: ['roberta_large_fc_0.803_ep7', 'roberta_large_fc_0.801_ep11(masked_ffmpeg_msgTF)', 'roberta_large_fc_0.799_ep6', 'roberta_large_fc_0.799_ep13', 'roberta_large_fc_0.798_ep15', 'roberta_large_fc_0.798_ep12', 'roberta_large_fc_0.796_ep9', 'roberta_large_fc_0.795_ep14', 'roberta_large_fc_0.794_ep10']\n",
      "\n",
      "#######################################roberta_large_fc_0.803_ep7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7595    0.9940    0.8611      1995\n",
      "  vulnerable     0.9863    0.5782    0.7290      1489\n",
      "\n",
      "    accuracy                         0.8163      3484\n",
      "   macro avg     0.8729    0.7861    0.7950      3484\n",
      "weighted avg     0.8564    0.8163    0.8046      3484\n",
      "\n",
      "[[1983   12]\n",
      " [ 628  861]]\n",
      "\n",
      "#######################################roberta_large_fc_0.801_ep11(masked_ffmpeg_msgTF)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7609    0.9920    0.8612      1995\n",
      "  vulnerable     0.9819    0.5823    0.7310      1489\n",
      "\n",
      "    accuracy                         0.8169      3484\n",
      "   macro avg     0.8714    0.7871    0.7961      3484\n",
      "weighted avg     0.8553    0.8169    0.8056      3484\n",
      "\n",
      "[[1979   16]\n",
      " [ 622  867]]\n",
      "\n",
      "#######################################roberta_large_fc_0.799_ep6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.813\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7584    0.9895    0.8586      1995\n",
      "  vulnerable     0.9762    0.5776    0.7257      1489\n",
      "\n",
      "    accuracy                         0.8134      3484\n",
      "   macro avg     0.8673    0.7835    0.7922      3484\n",
      "weighted avg     0.8514    0.8134    0.8018      3484\n",
      "\n",
      "[[1974   21]\n",
      " [ 629  860]]\n",
      "\n",
      "#######################################roberta_large_fc_0.799_ep13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7608    0.9900    0.8604      1995\n",
      "  vulnerable     0.9775    0.5829    0.7303      1489\n",
      "\n",
      "    accuracy                         0.8160      3484\n",
      "   macro avg     0.8691    0.7865    0.7954      3484\n",
      "weighted avg     0.8534    0.8160    0.8048      3484\n",
      "\n",
      "[[1975   20]\n",
      " [ 621  868]]\n",
      "\n",
      "#######################################roberta_large_fc_0.798_ep15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7610    0.9895    0.8603      1995\n",
      "  vulnerable     0.9764    0.5836    0.7306      1489\n",
      "\n",
      "    accuracy                         0.8160      3484\n",
      "   macro avg     0.8687    0.7865    0.7954      3484\n",
      "weighted avg     0.8531    0.8160    0.8049      3484\n",
      "\n",
      "[[1974   21]\n",
      " [ 620  869]]\n",
      "\n",
      "#######################################roberta_large_fc_0.798_ep12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7611    0.9915    0.8611      1995\n",
      "  vulnerable     0.9808    0.5829    0.7313      1489\n",
      "\n",
      "    accuracy                         0.8169      3484\n",
      "   macro avg     0.8709    0.7872    0.7962      3484\n",
      "weighted avg     0.8550    0.8169    0.8056      3484\n",
      "\n",
      "[[1978   17]\n",
      " [ 621  868]]\n",
      "\n",
      "#######################################roberta_large_fc_0.796_ep9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7596    0.9915    0.8602      1995\n",
      "  vulnerable     0.9807    0.5796    0.7286      1489\n",
      "\n",
      "    accuracy                         0.8154      3484\n",
      "   macro avg     0.8701    0.7855    0.7944      3484\n",
      "weighted avg     0.8541    0.8154    0.8039      3484\n",
      "\n",
      "[[1978   17]\n",
      " [ 626  863]]\n",
      "\n",
      "#######################################roberta_large_fc_0.795_ep14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.816\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7610    0.9895    0.8603      1995\n",
      "  vulnerable     0.9764    0.5836    0.7306      1489\n",
      "\n",
      "    accuracy                         0.8160      3484\n",
      "   macro avg     0.8687    0.7865    0.7954      3484\n",
      "weighted avg     0.8531    0.8160    0.8049      3484\n",
      "\n",
      "[[1974   21]\n",
      " [ 620  869]]\n",
      "\n",
      "#######################################roberta_large_fc_0.794_ep10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7595    0.9895    0.8594      1995\n",
      "  vulnerable     0.9763    0.5803    0.7279      1489\n",
      "\n",
      "    accuracy                         0.8146      3484\n",
      "   macro avg     0.8679    0.7849    0.7936      3484\n",
      "weighted avg     0.8522    0.8146    0.8032      3484\n",
      "\n",
      "[[1974   21]\n",
      " [ 625  864]]\n"
     ]
    }
   ],
   "source": [
    "check_point_files_list = ['roberta_large_fc_0.803_ep7', 'roberta_large_fc_0.801_ep11',\n",
    "                          'roberta_large_fc_0.799_ep6', 'roberta_large_fc_0.799_ep13',\n",
    "                          'roberta_large_fc_0.798_ep15', 'roberta_large_fc_0.798_ep12',\n",
    "                          'roberta_large_fc_0.796_ep9', 'roberta_large_fc_0.795_ep14',\n",
    "                          'roberta_large_fc_0.794_ep10']\n",
    "print('check_point_files_list:', check_point_files_list)\n",
    "\n",
    "df_test = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "\n",
    "for check_point_file in check_point_files_list:\n",
    "    print(f'\\n#######################################{check_point_file}')\n",
    "    check_point_file = f'{MODEL_SAVE_PATH}/{check_point_file}.pt'\n",
    "    model = BertClassifier()\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(check_point_file))\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
