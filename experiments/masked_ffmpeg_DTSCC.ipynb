{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# This source code file refers to:\n",
    "# https://github.com/ICL-ml4csec/VulBERTa\n",
    "# https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n",
    "# https://huggingface.co/docs/transformers/model_doc/roberta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "def write_to_file(text, path, mode='a'): # 'a': append; 'w': overwrite\n",
    "    with open(path, mode) as f:\n",
    "        f.write(text)\n",
    "\n",
    "def mkdir_if_not_exist(directory):\n",
    "    if not directory: return\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "def remove_file_if_exist(path):\n",
    "    if not path: return\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except:\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using', device)\n",
    "\n",
    "# The following randomization refers to: https://github.com/ICL-ml4csec/VulBERTa/blob/main/Finetuning_VulBERTa-MLP.ipynb\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['WANDB_MODE'] = 'dryrun'\n",
    "\n",
    "# -------------------------------------- start\n",
    "\n",
    "DATASET_NAME = 'ffmpeg'\n",
    "DATASET_MASKING = 'masked_'\n",
    "# DATASET_MASKING = ''\n",
    "\n",
    "codeTF_check_point = 'vulberta_0.6900_ep4.pt'\n",
    "msgTF_check_point = 'roberta_large_fc_0.798_ep12(masked_ffmpeg_msgTF).pt'\n",
    "\n",
    "# -------------------------------------- end\n",
    "\n",
    "pretrained_model_path = '/root/autodl-tmp/VulBERTa/'\n",
    "\n",
    "root_directory = '/root/autodl-tmp'\n",
    "dataset_directory = f'{root_directory}/output_dataset_1/{DATASET_MASKING}{DATASET_NAME}'\n",
    "init_train_path = f'{dataset_directory}/train.json'\n",
    "init_val_path = f'{dataset_directory}/val.json'\n",
    "init_test_path = f'{dataset_directory}/test.json'\n",
    "intermediate_directory = f'{root_directory}/intermediate/{DATASET_MASKING}{DATASET_NAME}'\n",
    "mkdir_if_not_exist(f'{root_directory}/intermediate')\n",
    "mkdir_if_not_exist(intermediate_directory)\n",
    "\n",
    "finetuned_ct_model_path = f'{root_directory}/codeTF_check_point/{DATASET_MASKING}{DATASET_NAME}/{codeTF_check_point}'\n",
    "intermediate_ct_train_path = f'{intermediate_directory}/ct_train.txt'\n",
    "intermediate_ct_val_path = f'{intermediate_directory}/ct_val.txt'\n",
    "intermediate_ct_test_path = f'{intermediate_directory}/ct_test.txt'\n",
    "\n",
    "finetuned_mt_model_path = f'{root_directory}/msgTF_check_point/{DATASET_MASKING}{DATASET_NAME}/{msgTF_check_point}'\n",
    "intermediate_mt_train_path = f'{intermediate_directory}/mt_train.txt'\n",
    "intermediate_mt_val_path = f'{intermediate_directory}/mt_val.txt'\n",
    "intermediate_mt_test_path = f'{intermediate_directory}/mt_test.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CodeTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import torch\n",
    "import sklearn\n",
    "import random\n",
    "import clang\n",
    "from clang import *\n",
    "from clang import cindex\n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM, RobertaForSequenceClassification\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from tokenizers.pre_tokenizers import PreTokenizer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import NormalizedString,PreTokenizedString\n",
    "from typing import List\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers import normalizers,decoders\n",
    "from tokenizers.normalizers import StripAccents, unicode_normalizer_from_str, Replace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "from tokenizers import processors,pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from sklearn import metrics\n",
    "\n",
    "# definitions\n",
    "class MyTokenizer:\n",
    "    cidx = cindex.Index.create()\n",
    "\n",
    "    def clang_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n",
    "        ## Tokkenize using clang\n",
    "        tok = []\n",
    "        tu = self.cidx.parse('tmp.c',\n",
    "                       args=[''],  \n",
    "                       unsaved_files=[('tmp.c', str(normalized_string.original))],  \n",
    "                       options=0)\n",
    "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
    "            spelling = t.spelling.strip()\n",
    "            if spelling == '': continue\n",
    "            ## Keyword no need\n",
    "            ## Punctuations no need\n",
    "            ## Literal all to BPE\n",
    "            #spelling = spelling.replace(' ', '')\n",
    "            tok.append(NormalizedString(spelling))\n",
    "        return(tok)\n",
    "\n",
    "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
    "        pretok.split(self.clang_split)\n",
    "\n",
    "def process_encodings(encodings):\n",
    "    input_ids=[]\n",
    "    attention_mask=[]\n",
    "    for enc in encodings:\n",
    "        input_ids.append(enc.ids)\n",
    "        attention_mask.append(enc.attention_mask)\n",
    "    return {'input_ids':input_ids, 'attention_mask':attention_mask}\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# tokenize and load dataset\n",
    "print('Tokenizing dataset...')\n",
    "vocab, merges = BPE.read_file(vocab=\"./tokenizer/drapgh-vocab.json\", merges=\"./tokenizer/drapgh-merges.txt\")\n",
    "my_tokenizer = Tokenizer(BPE(vocab, merges, unk_token=\"<unk>\"))\n",
    "\n",
    "my_tokenizer.normalizer = normalizers.Sequence([StripAccents(), Replace(\" \", \"Ä\")])\n",
    "my_tokenizer.pre_tokenizer = PreTokenizer.custom(MyTokenizer())\n",
    "my_tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "my_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"<s> $A </s>\",\n",
    "    special_tokens=[\n",
    "    (\"<s>\",0),\n",
    "    (\"<pad>\",1),\n",
    "    (\"</s>\",2),\n",
    "    (\"<unk>\",3),\n",
    "    (\"<mask>\",4)\n",
    "    ]\n",
    ")\n",
    "\n",
    "my_tokenizer.enable_truncation(max_length=1024)\n",
    "my_tokenizer.enable_padding(direction='right', pad_id=1, pad_type_id=0, pad_token='<pad>', length=None, pad_to_multiple_of=None)\n",
    "\n",
    "m1 = pd.read_json(init_train_path)\n",
    "m2 = pd.read_json(init_val_path)\n",
    "# m3 = pd.read_json(init_test_path)\n",
    "\n",
    "train_encodings = my_tokenizer.encode_batch(m1.commit_patch)\n",
    "train_encodings = process_encodings(train_encodings)\n",
    "\n",
    "val_encodings = my_tokenizer.encode_batch(m2.commit_patch)\n",
    "val_encodings = process_encodings(val_encodings)\n",
    "\n",
    "# test_encodings = my_tokenizer.encode_batch(m3.commit_patch)\n",
    "# test_encodings = process_encodings(test_encodings)\n",
    "\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        assert len(self.encodings['input_ids']) == len(self.encodings['attention_mask']) == len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = MyCustomDataset(train_encodings, m1.label.tolist())\n",
    "val_dataset = MyCustomDataset(val_encodings, m2.label.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating intermediate data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /root/autodl-tmp/VulBERTa/ were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /root/autodl-tmp/VulBERTa/ and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 82/82 [02:27<00:00,  1.80s/it]\n",
      "100%|██████████| 28/28 [00:49<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.6900\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7220    0.7459    0.7337      1995\n",
      "  vulnerable     0.6437    0.6152    0.6291      1489\n",
      "\n",
      "    accuracy                         0.6900      3484\n",
      "   macro avg     0.6828    0.6805    0.6814      3484\n",
      "weighted avg     0.6885    0.6900    0.6890      3484\n",
      "\n",
      "[[1488  507]\n",
      " [ 573  916]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# generate intermediate data by CodeTransformer\n",
    "from sklearn import metrics\n",
    "\n",
    "pretrained_model_path = '/root/autodl-tmp/VulBERTa/'\n",
    "\n",
    "print('Generating intermediate data...')\n",
    "model = RobertaForSequenceClassification.from_pretrained(pretrained_model_path)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(finetuned_ct_model_path))\n",
    "\n",
    "def generate_ct_intermediate_dataset(input_data, intermediate_data_path, evaluate=True):\n",
    "    data_loader = DataLoader(input_data, batch_size=128)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    total_acc = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            # outputs['logits'] is equal to outputs[0]\n",
    "            outputs = outputs['logits']\n",
    "\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1).tolist()\n",
    "            assert(len(probs) == len(labels))\n",
    "            for i in range(len(probs)):\n",
    "                prob = probs[i]\n",
    "                label = int(labels[i])\n",
    "                content = '\\t'.join([str(i) for i in prob + [label]]) + '\\n'\n",
    "                write_to_file(content, intermediate_data_path)\n",
    "\n",
    "            if evaluate:\n",
    "                acc = (outputs.argmax(dim=1) == labels).sum().item()\n",
    "                total_acc += acc\n",
    "\n",
    "                labels = labels.data.cpu().numpy()\n",
    "                predic = outputs.argmax(dim=1).data.cpu().numpy()\n",
    "                labels_all = np.append(labels_all, labels)\n",
    "                predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    if evaluate:\n",
    "        report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "        confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "        print(f'Test Accuracy: {total_acc / len(input_data): .4f}')\n",
    "        print(report)\n",
    "        print(confusion)\n",
    "\n",
    "remove_file_if_exist(intermediate_ct_train_path)\n",
    "remove_file_if_exist(intermediate_ct_val_path)\n",
    "\n",
    "generate_ct_intermediate_dataset(train_dataset, intermediate_ct_train_path, False)\n",
    "generate_ct_intermediate_dataset(val_dataset, intermediate_ct_val_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MsgTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 82/82 [02:54<00:00,  2.13s/it]\n",
      "100%|██████████| 28/28 [00:58<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7611    0.9915    0.8611      1995\n",
      "  vulnerable     0.9808    0.5829    0.7313      1489\n",
      "\n",
      "    accuracy                         0.8169      3484\n",
      "   macro avg     0.8709    0.7872    0.7962      3484\n",
      "weighted avg     0.8550    0.8169    0.8056      3484\n",
      "\n",
      "[[1978   17]\n",
      " [ 621  868]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from transformers import RobertaModel, RobertaTokenizerFast\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import os\n",
    "import random\n",
    "\n",
    "# definitions\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "BERT_CONFIG = 'roberta-large'\n",
    "labels = {0:0, 1:1}\n",
    "BATCH_SIZE = 128\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(BERT_CONFIG)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['label']]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['commit_message']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "    \n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = RobertaModel.from_pretrained(BERT_CONFIG)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if BERT_CONFIG == 'roberta-large':\n",
    "            self.linear = nn.Linear(1024, len(labels))\n",
    "        else:\n",
    "            self.linear = nn.Linear(768, len(labels))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        # final_layer = self.relu(linear_output) # IMPO CHANGE\n",
    "        return linear_output\n",
    "\n",
    "    def check_parameters(self):\n",
    "        print('The number of Bert parameters:', self.bert.num_parameters())\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# generate intermediate data by MsgTransformer\n",
    "model = BertClassifier()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(finetuned_mt_model_path))\n",
    "\n",
    "def generate_mt_intermediate_dataset(input_data, intermediate_data_path):\n",
    "    data_loader = torch.utils.data.DataLoader(Dataset(input_data), batch_size=BATCH_SIZE)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in tqdm(data_loader):\n",
    "            labels = labels.to(device)\n",
    "            masks = texts['attention_mask'].to(device)\n",
    "            input_ids = texts['input_ids'].squeeze(1).to(device)\n",
    "            outputs = model(input_ids, masks)\n",
    "\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1).tolist()\n",
    "            assert(len(probs) == len(labels))\n",
    "            for i in range(len(probs)):\n",
    "                prob = probs[i]\n",
    "                label = int(labels[i])\n",
    "                content = '\\t'.join([str(i) for i in prob + [label]]) + '\\n'\n",
    "                write_to_file(content, intermediate_data_path)\n",
    "\n",
    "df_train = pd.read_json(init_train_path)\n",
    "df_val = pd.read_json(init_val_path)\n",
    "\n",
    "remove_file_if_exist(intermediate_mt_train_path)\n",
    "remove_file_if_exist(intermediate_mt_val_path)\n",
    "\n",
    "generate_mt_intermediate_dataset(df_train, intermediate_mt_train_path)\n",
    "generate_mt_intermediate_dataset(df_val, intermediate_mt_val_path)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# evaluation\n",
    "print('\\nEvaluation:')\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    total_acc_test = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            test_label = test_label.data.cpu().numpy()\n",
    "            predic = output.argmax(dim=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, test_label)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    print(report)\n",
    "    print(confusion)\n",
    "\n",
    "model = BertClassifier()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(finetuned_mt_model_path))\n",
    "evaluate(model, df_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Combine everything into intermediate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_train_path = f'{intermediate_directory}/train.txt'\n",
    "intermediate_val_path = f'{intermediate_directory}/val.txt'\n",
    "\n",
    "def generate_intermediate_dataset(intermediate_mt_data_path, intermediate_ct_data_path, intermediate_data_path):\n",
    "    with open(intermediate_mt_data_path) as f:\n",
    "        mt_data_list = f.read().split('\\n')\n",
    "    \n",
    "    with open(intermediate_ct_data_path) as f:\n",
    "        ct_data_list = f.read().split('\\n')\n",
    "    \n",
    "    mt_data_list = mt_data_list[:-1] if not mt_data_list[-1] else mt_data_list\n",
    "    ct_data_list = ct_data_list[:-1] if not ct_data_list[-1] else ct_data_list\n",
    "\n",
    "    assert(len(mt_data_list) == len(ct_data_list))\n",
    "    \n",
    "    for i in range(len(mt_data_list)):\n",
    "        mt_data = mt_data_list[i].split('\\t')\n",
    "        ct_data = ct_data_list[i].split('\\t')\n",
    "        assert(mt_data[2] == ct_data[2])\n",
    "        label = mt_data[2]\n",
    "        content = '\\t'.join(mt_data[:2] + ct_data[:2] + [label])\n",
    "        content = content + '\\n' if i < len(mt_data_list) - 1 else content\n",
    "        write_to_file(content, intermediate_data_path)\n",
    "\n",
    "remove_file_if_exist(intermediate_train_path)\n",
    "remove_file_if_exist(intermediate_val_path)\n",
    "\n",
    "generate_intermediate_dataset(intermediate_mt_train_path, intermediate_ct_train_path, intermediate_train_path)\n",
    "generate_intermediate_dataset(intermediate_mt_val_path, intermediate_ct_val_path, intermediate_val_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 623.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.1477             | Train Accuracy:  0.7870             | Val Loss:  0.1520             | Val Accuracy:  0.7270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 600.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.1467             | Train Accuracy:  0.7917             | Val Loss:  0.1512             | Val Accuracy:  0.7276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 613.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.1454             | Train Accuracy:  0.7927             | Val Loss:  0.1505             | Val Accuracy:  0.7285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 621.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.1444             | Train Accuracy:  0.8052             | Val Loss:  0.1497             | Val Accuracy:  0.7288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 638.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.1435             | Train Accuracy:  0.8332             | Val Loss:  0.1489             | Val Accuracy:  0.7299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 598.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.1423             | Train Accuracy:  0.8396             | Val Loss:  0.1482             | Val Accuracy:  0.7305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 605.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.1413             | Train Accuracy:  0.8391             | Val Loss:  0.1474             | Val Accuracy:  0.7305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 613.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.1404             | Train Accuracy:  0.8356             | Val Loss:  0.1466             | Val Accuracy:  0.7308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 561.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.1393             | Train Accuracy:  0.8423             | Val Loss:  0.1459             | Val Accuracy:  0.7750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 647.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.1380             | Train Accuracy:  0.8471             | Val Loss:  0.1451             | Val Accuracy:  0.7781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 613.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.1371             | Train Accuracy:  0.8475             | Val Loss:  0.1443             | Val Accuracy:  0.7799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 604.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.1362             | Train Accuracy:  0.8405             | Val Loss:  0.1436             | Val Accuracy:  0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 590.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.1352             | Train Accuracy:  0.8455             | Val Loss:  0.1428             | Val Accuracy:  0.7796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 637.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.1339             | Train Accuracy:  0.8487             | Val Loss:  0.1421             | Val Accuracy:  0.7801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 661.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.1328             | Train Accuracy:  0.8461             | Val Loss:  0.1413             | Val Accuracy:  0.7804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 646.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 16 | Train Loss:  0.1320             | Train Accuracy:  0.8495             | Val Loss:  0.1405             | Val Accuracy:  0.7801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 648.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 17 | Train Loss:  0.1306             | Train Accuracy:  0.8557             | Val Loss:  0.1398             | Val Accuracy:  0.7799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 615.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 18 | Train Loss:  0.1297             | Train Accuracy:  0.8590             | Val Loss:  0.1390             | Val Accuracy:  0.7804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 548.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 19 | Train Loss:  0.1289             | Train Accuracy:  0.8542             | Val Loss:  0.1383             | Val Accuracy:  0.7807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 627.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 20 | Train Loss:  0.1278             | Train Accuracy:  0.8586             | Val Loss:  0.1376             | Val Accuracy:  0.7816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 633.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 21 | Train Loss:  0.1270             | Train Accuracy:  0.8615             | Val Loss:  0.1368             | Val Accuracy:  0.7819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:05<00:00, 517.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 22 | Train Loss:  0.1257             | Train Accuracy:  0.8633             | Val Loss:  0.1361             | Val Accuracy:  0.7819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 563.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 23 | Train Loss:  0.1247             | Train Accuracy:  0.8636             | Val Loss:  0.1354             | Val Accuracy:  0.7819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 542.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 24 | Train Loss:  0.1237             | Train Accuracy:  0.8636             | Val Loss:  0.1346             | Val Accuracy:  0.7819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 620.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 25 | Train Loss:  0.1224             | Train Accuracy:  0.8696             | Val Loss:  0.1339             | Val Accuracy:  0.7830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 626.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 26 | Train Loss:  0.1218             | Train Accuracy:  0.8638             | Val Loss:  0.1331             | Val Accuracy:  0.7833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 633.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 27 | Train Loss:  0.1205             | Train Accuracy:  0.8644             | Val Loss:  0.1324             | Val Accuracy:  0.7836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 671.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 28 | Train Loss:  0.1197             | Train Accuracy:  0.8714             | Val Loss:  0.1317             | Val Accuracy:  0.7842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 598.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 29 | Train Loss:  0.1188             | Train Accuracy:  0.8737             | Val Loss:  0.1310             | Val Accuracy:  0.7844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 614.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 30 | Train Loss:  0.1175             | Train Accuracy:  0.8774             | Val Loss:  0.1303             | Val Accuracy:  0.7842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 562.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 31 | Train Loss:  0.1166             | Train Accuracy:  0.8769             | Val Loss:  0.1296             | Val Accuracy:  0.7847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 648.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 32 | Train Loss:  0.1153             | Train Accuracy:  0.8768             | Val Loss:  0.1288             | Val Accuracy:  0.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 552.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 33 | Train Loss:  0.1145             | Train Accuracy:  0.8803             | Val Loss:  0.1281             | Val Accuracy:  0.7850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 614.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 34 | Train Loss:  0.1137             | Train Accuracy:  0.8805             | Val Loss:  0.1274             | Val Accuracy:  0.7859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 642.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 35 | Train Loss:  0.1128             | Train Accuracy:  0.8854             | Val Loss:  0.1267             | Val Accuracy:  0.7873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 644.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 36 | Train Loss:  0.1117             | Train Accuracy:  0.8841             | Val Loss:  0.1260             | Val Accuracy:  0.7867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 654.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 37 | Train Loss:  0.1108             | Train Accuracy:  0.8849             | Val Loss:  0.1254             | Val Accuracy:  0.7867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 653.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 38 | Train Loss:  0.1097             | Train Accuracy:  0.8858             | Val Loss:  0.1247             | Val Accuracy:  0.7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 530.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 39 | Train Loss:  0.1090             | Train Accuracy:  0.8868             | Val Loss:  0.1240             | Val Accuracy:  0.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 611.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 40 | Train Loss:  0.1079             | Train Accuracy:  0.8909             | Val Loss:  0.1233             | Val Accuracy:  0.7890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 626.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 41 | Train Loss:  0.1068             | Train Accuracy:  0.8897             | Val Loss:  0.1227             | Val Accuracy:  0.7899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 674.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 42 | Train Loss:  0.1060             | Train Accuracy:  0.8931             | Val Loss:  0.1220             | Val Accuracy:  0.7905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 596.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 43 | Train Loss:  0.1048             | Train Accuracy:  0.8921             | Val Loss:  0.1214             | Val Accuracy:  0.7916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 653.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 44 | Train Loss:  0.1041             | Train Accuracy:  0.8960             | Val Loss:  0.1207             | Val Accuracy:  0.7922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 634.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 45 | Train Loss:  0.1032             | Train Accuracy:  0.8968             | Val Loss:  0.1201             | Val Accuracy:  0.7936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 661.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 46 | Train Loss:  0.1020             | Train Accuracy:  0.8973             | Val Loss:  0.1195             | Val Accuracy:  0.7939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 571.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 47 | Train Loss:  0.1015             | Train Accuracy:  0.8983             | Val Loss:  0.1189             | Val Accuracy:  0.7942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 523.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 48 | Train Loss:  0.1006             | Train Accuracy:  0.9004             | Val Loss:  0.1183             | Val Accuracy:  0.7954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 529.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 49 | Train Loss:  0.0997             | Train Accuracy:  0.9002             | Val Loss:  0.1177             | Val Accuracy:  0.7962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 597.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 50 | Train Loss:  0.0985             | Train Accuracy:  0.9022             | Val Loss:  0.1171             | Val Accuracy:  0.7974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 627.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 51 | Train Loss:  0.0979             | Train Accuracy:  0.9026             | Val Loss:  0.1165             | Val Accuracy:  0.7974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 604.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 52 | Train Loss:  0.0971             | Train Accuracy:  0.9059             | Val Loss:  0.1159             | Val Accuracy:  0.7979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 590.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 53 | Train Loss:  0.0963             | Train Accuracy:  0.9048             | Val Loss:  0.1154             | Val Accuracy:  0.7982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 658.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 54 | Train Loss:  0.0951             | Train Accuracy:  0.9069             | Val Loss:  0.1148             | Val Accuracy:  0.7985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 665.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 55 | Train Loss:  0.0944             | Train Accuracy:  0.9074             | Val Loss:  0.1143             | Val Accuracy:  0.7994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 665.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 56 | Train Loss:  0.0936             | Train Accuracy:  0.9120             | Val Loss:  0.1137             | Val Accuracy:  0.8002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 678.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 57 | Train Loss:  0.0925             | Train Accuracy:  0.9125             | Val Loss:  0.1132             | Val Accuracy:  0.8005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 567.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 58 | Train Loss:  0.0921             | Train Accuracy:  0.9188             | Val Loss:  0.1127             | Val Accuracy:  0.8011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 601.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 59 | Train Loss:  0.0916             | Train Accuracy:  0.9204             | Val Loss:  0.1122             | Val Accuracy:  0.8028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 674.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 60 | Train Loss:  0.0902             | Train Accuracy:  0.9235             | Val Loss:  0.1117             | Val Accuracy:  0.8037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 665.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 61 | Train Loss:  0.0895             | Train Accuracy:  0.9237             | Val Loss:  0.1112             | Val Accuracy:  0.8045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 599.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 62 | Train Loss:  0.0891             | Train Accuracy:  0.9213             | Val Loss:  0.1107             | Val Accuracy:  0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 634.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 63 | Train Loss:  0.0883             | Train Accuracy:  0.9212             | Val Loss:  0.1102             | Val Accuracy:  0.8152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 671.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 64 | Train Loss:  0.0873             | Train Accuracy:  0.9227             | Val Loss:  0.1098             | Val Accuracy:  0.8628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 663.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 65 | Train Loss:  0.0867             | Train Accuracy:  0.9229             | Val Loss:  0.1093             | Val Accuracy:  0.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 675.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 66 | Train Loss:  0.0862             | Train Accuracy:  0.9229             | Val Loss:  0.1089             | Val Accuracy:  0.8645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 677.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 67 | Train Loss:  0.0855             | Train Accuracy:  0.9247             | Val Loss:  0.1084             | Val Accuracy:  0.8648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 668.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 68 | Train Loss:  0.0848             | Train Accuracy:  0.9238             | Val Loss:  0.1080             | Val Accuracy:  0.8651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 665.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 69 | Train Loss:  0.0842             | Train Accuracy:  0.9240             | Val Loss:  0.1076             | Val Accuracy:  0.8651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:03<00:00, 660.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 70 | Train Loss:  0.0835             | Train Accuracy:  0.9243             | Val Loss:  0.1072             | Val Accuracy:  0.8654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 575.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 71 | Train Loss:  0.0831             | Train Accuracy:  0.9243             | Val Loss:  0.1068             | Val Accuracy:  0.8648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 613.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 72 | Train Loss:  0.0820             | Train Accuracy:  0.9253             | Val Loss:  0.1064             | Val Accuracy:  0.8648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 650.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 73 | Train Loss:  0.0817             | Train Accuracy:  0.9260             | Val Loss:  0.1060             | Val Accuracy:  0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:05<00:00, 517.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 74 | Train Loss:  0.0807             | Train Accuracy:  0.9275             | Val Loss:  0.1056             | Val Accuracy:  0.8639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 581.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 75 | Train Loss:  0.0805             | Train Accuracy:  0.9248             | Val Loss:  0.1052             | Val Accuracy:  0.8634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 644.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 76 | Train Loss:  0.0796             | Train Accuracy:  0.9244             | Val Loss:  0.1049             | Val Accuracy:  0.8634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 602.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 77 | Train Loss:  0.0792             | Train Accuracy:  0.9281             | Val Loss:  0.1045             | Val Accuracy:  0.8637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 617.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 78 | Train Loss:  0.0785             | Train Accuracy:  0.9285             | Val Loss:  0.1042             | Val Accuracy:  0.8637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 629.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 79 | Train Loss:  0.0780             | Train Accuracy:  0.9295             | Val Loss:  0.1038             | Val Accuracy:  0.8637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [00:04<00:00, 593.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 80 | Train Loss:  0.0773             | Train Accuracy:  0.9294             | Val Loss:  0.1035             | Val Accuracy:  0.8637\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "EPOCHS = 80\n",
    "LR = 1e-6\n",
    "BATCH_SIZE = 4\n",
    "intermediate_train_path = f'{intermediate_directory}/train.txt'\n",
    "intermediate_val_path = f'{intermediate_directory}/val.txt'\n",
    "MODEL_SAVE_PATH = f'{root_directory}/ensemble_model/{DATASET_MASKING}{DATASET_NAME}'\n",
    "mkdir_if_not_exist(f'{root_directory}/ensemble_model')\n",
    "remove_file_if_exist(MODEL_SAVE_PATH)\n",
    "mkdir_if_not_exist(MODEL_SAVE_PATH)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(path) as f:\n",
    "            data_list = f.read().split('\\n')\n",
    "        self.labels = [ int(data.split('\\t')[-1]) for data in data_list ]\n",
    "        self.inputs = [ [float(v) for v in data.split('\\t')[:-1]] for data in data_list ]\n",
    "        assert(len(self.labels) == len(self.inputs))\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.inputs[idx]\n",
    "        y = self.labels[idx]\n",
    "        return x[0], x[1], x[2], x[3], y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc1 = nn.Linear(input_dim, 20)\n",
    "        self.out = nn.Linear(20, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "def train(model, train_dataset, val_dataset):\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "        for x1, x2, x3, x4, y in tqdm(train_dataloader):\n",
    "            x = torch.transpose(torch.stack([x1, x2, x3, x4]), 0, 1).float().to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            total_loss_train += loss.item()\n",
    "\n",
    "            acc = (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, x3, x4, y in val_dataloader:\n",
    "                x = torch.transpose(torch.stack([x1, x2, x3, x4]), 0, 1).float().to(device)\n",
    "                y = y.to(device)\n",
    "                y_pred = model(x)\n",
    "\n",
    "                loss = criterion(y_pred, y)\n",
    "                total_loss_val += loss.item()\n",
    "\n",
    "                acc = (y_pred.argmax(dim=1) == y).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_dataset): .4f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_dataset): .4f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_dataset): .4f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_dataset): .4f}')\n",
    "\n",
    "        val_acc = f'{total_acc_val / len(val_dataset):.4f}'\n",
    "        torch.save(model.state_dict(), f'{MODEL_SAVE_PATH}/ensemble2_{val_acc}_epoch{epoch_num + 1}.pt')\n",
    "\n",
    "train_dataset = MyDataset(intermediate_train_path)\n",
    "val_dataset = MyDataset(intermediate_val_path)\n",
    "\n",
    "model = MLP(4, 2)\n",
    "train(model, train_dataset, val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.8654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.8707    0.8982    0.8843      1995\n",
      "  vulnerable     0.8576    0.8214    0.8391      1489\n",
      "\n",
      "    accuracy                         0.8654      3484\n",
      "   macro avg     0.8642    0.8598    0.8617      3484\n",
      "weighted avg     0.8651    0.8654    0.8650      3484\n",
      "\n",
      "[[1792  203]\n",
      " [ 266 1223]]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_dataset):\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_acc_test = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, x3, x4, y in test_dataloader:\n",
    "            x = torch.transpose(torch.stack([x1, x2, x3, x4]), 0, 1).float().to(device)\n",
    "            y = y.to(device)\n",
    "            y_pred = model(x)\n",
    "            \n",
    "            acc = (y_pred.argmax(dim=1) == y).sum().item()\n",
    "            total_acc_test += acc\n",
    "            \n",
    "            y = y.data.cpu().numpy()\n",
    "            predic = y_pred.argmax(dim=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, y)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_dataset): .4f}')\n",
    "    print(report)\n",
    "    print(confusion)\n",
    "\n",
    "mkdir_if_not_exist(MODEL_SAVE_PATH)\n",
    "model = MLP(4, 2)\n",
    "saved_model_name = 'ensemble2_0.8654_epoch70.pt'\n",
    "model.load_state_dict(torch.load(f'{MODEL_SAVE_PATH}/{saved_model_name}'))\n",
    "evaluate(model, val_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
