{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# This source code file refers to:\n",
    "# https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n",
    "# https://huggingface.co/docs/transformers/model_doc/roberta\n",
    "# https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/1_pretrained_vectors.ipynb\n",
    "# https://github.com/dpressel/mead-baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import os\n",
    "import random\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments, RobertaModel\n",
    "\n",
    "# The following randomization refers to: https://github.com/ICL-ml4csec/VulBERTa/blob/main/Finetuning_VulBERTa-MLP.ipynb\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "MODEL_SAVE_PATH = '/root/autodl-tmp/finetuned_models'\n",
    "DATASET_ROOT_PATH = '/root/autodl-tmp'\n",
    "BERT_CONFIG = 'roberta-large' # roberta-base, roberta-large\n",
    "MODEL_NAME = 'qemu'\n",
    "labels = {0:0, 1:1}\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "ONLY_TESTING = False\n",
    "\n",
    "DATASET_NAME = 'qemu'\n",
    "# DATASET_MASKING = '_masked'\n",
    "DATASET_MASKING = ''\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 15\n",
    "LR = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "MODEL_SAVE_PATH: /root/autodl-tmp/finetuned_models/qemu\n",
      "DATASET_PATH: /root/autodl-tmp/output_dataset_1/qemu\n",
      "BERT_CONFIG: roberta-large\n",
      "ONLY_TESTING: False\n",
      "DATASET_NAME: qemu\n",
      "DATASET_MASKING: \n",
      "BATCH_SIZE: 4\n",
      "EPOCHS: 15\n",
      "LR: 1e-06\n",
      "using device: cuda\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(BERT_CONFIG)\n",
    "DATASET_PATH = f'{DATASET_ROOT_PATH}/output_dataset_1/{DATASET_NAME}{DATASET_MASKING}'\n",
    "MODEL_SAVE_PATH = f'{MODEL_SAVE_PATH}/{DATASET_NAME}{DATASET_MASKING}'\n",
    "\n",
    "print('seed:', seed)\n",
    "print('MODEL_SAVE_PATH:', MODEL_SAVE_PATH)\n",
    "print('DATASET_PATH:', DATASET_PATH)\n",
    "print('BERT_CONFIG:', BERT_CONFIG)\n",
    "\n",
    "print('ONLY_TESTING:', ONLY_TESTING)\n",
    "print('DATASET_NAME:', DATASET_NAME)\n",
    "print('DATASET_MASKING:', DATASET_MASKING)\n",
    "\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "print('EPOCHS:', EPOCHS)\n",
    "print('LR:', LR)\n",
    "\n",
    "print('using device:', device)\n",
    "print('GPU count:', torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "mkdir_if_not_exist(MODEL_SAVE_PATH)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['label']]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['commit_message']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "        bert_config = 'large' if BERT_CONFIG == 'roberta-large' else 'base'\n",
    "        val_acc = f'{total_acc_val / len(val_data):.3f}'\n",
    "        torch.save(model.state_dict(), f'{MODEL_SAVE_PATH}/roberta_{bert_config}_{MODEL_NAME}_{val_acc}_ep{epoch_num + 1}.pt')\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    total_acc_test = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            test_label = test_label.data.cpu().numpy()\n",
    "            predic = output.argmax(dim=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, test_label)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    print(report)\n",
    "    print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ParallelConv(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, filters, dropout=0.5):\n",
    "        super().__init__()\n",
    "        convs = []        \n",
    "        self.output_dims = sum([t[1] for t in filters])\n",
    "        for (filter_length, output_dims) in filters:\n",
    "            pad = filter_length//2\n",
    "            conv = nn.Sequential(\n",
    "                nn.Conv1d(input_dims, output_dims, filter_length, padding=pad),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            convs.append(conv)\n",
    "        # Add the module so its managed correctly\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.conv_drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_bct):\n",
    "        mots = []\n",
    "        for conv in self.convs:\n",
    "            # In Conv1d, data BxCxT, max over time\n",
    "            conv_out = conv(input_bct)\n",
    "            mot, _ = conv_out.max(2)\n",
    "            mots.append(mot)\n",
    "        mots = torch.cat(mots, 1)\n",
    "        return self.conv_drop(mots)\n",
    "\n",
    "class ConvClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dims,\n",
    "                 filters=[(2, 100), (3, 100), (4, 100)],\n",
    "                 dropout=0.5, hidden_units=[]):\n",
    "        super().__init__()\n",
    "        self.bert = RobertaModel.from_pretrained(BERT_CONFIG)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.convs = ParallelConv(embed_dims, filters, dropout)\n",
    "        \n",
    "        input_units = self.convs.output_dims\n",
    "        output_units = self.convs.output_dims\n",
    "        sequence = []\n",
    "        for h in hidden_units:\n",
    "            sequence.append(self.dropout(nn.Linear(input_units, h)))\n",
    "            input_units = h\n",
    "            output_units = h\n",
    "            \n",
    "        sequence.append(nn.Linear(output_units, 2))\n",
    "        self.outputs = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        x, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        embed = self.dropout(x)\n",
    "        embed = embed.transpose(1, 2).contiguous()\n",
    "        hidden = self.convs(embed)\n",
    "        linear = self.outputs(hidden)\n",
    "        return F.log_softmax(linear, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 2227/2227 [09:12<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.199             | Train Accuracy:  0.524             | Val Loss:  0.184             | Val Accuracy:  0.576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.089             | Train Accuracy:  0.849             | Val Loss:  0.048             | Val Accuracy:  0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.036             | Train Accuracy:  0.952             | Val Loss:  0.041             | Val Accuracy:  0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.021             | Train Accuracy:  0.977             | Val Loss:  0.031             | Val Accuracy:  0.966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.015             | Train Accuracy:  0.985             | Val Loss:  0.031             | Val Accuracy:  0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.010             | Train Accuracy:  0.991             | Val Loss:  0.037             | Val Accuracy:  0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.007             | Train Accuracy:  0.993             | Val Loss:  0.040             | Val Accuracy:  0.967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.003             | Train Accuracy:  0.997             | Val Loss:  0.043             | Val Accuracy:  0.972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.004             | Train Accuracy:  0.997             | Val Loss:  0.042             | Val Accuracy:  0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.002             | Train Accuracy:  0.998             | Val Loss:  0.042             | Val Accuracy:  0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.003             | Train Accuracy:  0.997             | Val Loss:  0.044             | Val Accuracy:  0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.001             | Train Accuracy:  0.999             | Val Loss:  0.045             | Val Accuracy:  0.976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.000             | Train Accuracy:  1.000             | Val Loss:  0.048             | Val Accuracy:  0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.002             | Train Accuracy:  0.998             | Val Loss:  0.039             | Val Accuracy:  0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:14<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.002             | Train Accuracy:  0.999             | Val Loss:  0.035             | Val Accuracy:  0.972\n",
      "Test Accuracy:  0.971\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9610    0.9914    0.9759      1738\n",
      "  vulnerable     0.9872    0.9431    0.9647      1231\n",
      "\n",
      "    accuracy                         0.9714      2969\n",
      "   macro avg     0.9741    0.9673    0.9703      2969\n",
      "weighted avg     0.9719    0.9714    0.9713      2969\n",
      "\n",
      "[[1723   15]\n",
      " [  70 1161]]\n"
     ]
    }
   ],
   "source": [
    "if not ONLY_TESTING:\n",
    "    embed_dim = 1024\n",
    "    model = ConvClassifier(embed_dim)\n",
    "    model.to(device)\n",
    "\n",
    "    df_train = pd.read_json(f'{DATASET_PATH}/train.json')\n",
    "    df_val = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "    df_test = pd.read_json(f'{DATASET_PATH}/test.json')\n",
    "    train(model, df_train, df_val, LR, EPOCHS)\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_files_list: ['roberta_large_qemu_0.976_ep12', 'roberta_large_qemu_0.975_ep14', 'roberta_large_qemu_0.975_ep13', 'roberta_large_qemu_0.973_ep9', 'roberta_large_qemu_0.973_ep10', 'roberta_large_qemu_0.972_ep8', 'roberta_large_qemu_0.972_ep15']\n",
      "\n",
      "#######################################roberta_large_qemu_0.976_ep12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9749    0.9839    0.9794      1738\n",
      "  vulnerable     0.9770    0.9643    0.9706      1231\n",
      "\n",
      "    accuracy                         0.9757      2969\n",
      "   macro avg     0.9759    0.9741    0.9750      2969\n",
      "weighted avg     0.9758    0.9757    0.9757      2969\n",
      "\n",
      "[[1710   28]\n",
      " [  44 1187]]\n",
      "\n",
      "#######################################roberta_large_qemu_0.975_ep14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9722    0.9862    0.9791      1738\n",
      "  vulnerable     0.9801    0.9602    0.9700      1231\n",
      "\n",
      "    accuracy                         0.9754      2969\n",
      "   macro avg     0.9762    0.9732    0.9746      2969\n",
      "weighted avg     0.9755    0.9754    0.9754      2969\n",
      "\n",
      "[[1714   24]\n",
      " [  49 1182]]\n",
      "\n",
      "#######################################roberta_large_qemu_0.975_ep13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9722    0.9862    0.9791      1738\n",
      "  vulnerable     0.9801    0.9602    0.9700      1231\n",
      "\n",
      "    accuracy                         0.9754      2969\n",
      "   macro avg     0.9762    0.9732    0.9746      2969\n",
      "weighted avg     0.9755    0.9754    0.9754      2969\n",
      "\n",
      "[[1714   24]\n",
      " [  49 1182]]\n",
      "\n",
      "#######################################roberta_large_qemu_0.973_ep9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9694    0.9827    0.9760      1738\n",
      "  vulnerable     0.9751    0.9561    0.9655      1231\n",
      "\n",
      "    accuracy                         0.9717      2969\n",
      "   macro avg     0.9722    0.9694    0.9708      2969\n",
      "weighted avg     0.9718    0.9717    0.9717      2969\n",
      "\n",
      "[[1708   30]\n",
      " [  54 1177]]\n",
      "\n",
      "#######################################roberta_large_qemu_0.973_ep10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.973\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9689    0.9856    0.9772      1738\n",
      "  vulnerable     0.9792    0.9553    0.9671      1231\n",
      "\n",
      "    accuracy                         0.9731      2969\n",
      "   macro avg     0.9740    0.9705    0.9721      2969\n",
      "weighted avg     0.9732    0.9731    0.9730      2969\n",
      "\n",
      "[[1713   25]\n",
      " [  55 1176]]\n",
      "\n",
      "#######################################roberta_large_qemu_0.972_ep8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9636    0.9902    0.9767      1738\n",
      "  vulnerable     0.9856    0.9472    0.9660      1231\n",
      "\n",
      "    accuracy                         0.9724      2969\n",
      "   macro avg     0.9746    0.9687    0.9714      2969\n",
      "weighted avg     0.9727    0.9724    0.9723      2969\n",
      "\n",
      "[[1721   17]\n",
      " [  65 1166]]\n",
      "\n",
      "#######################################roberta_large_qemu_0.972_ep15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.971\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9604    0.9914    0.9757      1738\n",
      "  vulnerable     0.9872    0.9423    0.9643      1231\n",
      "\n",
      "    accuracy                         0.9710      2969\n",
      "   macro avg     0.9738    0.9668    0.9700      2969\n",
      "weighted avg     0.9715    0.9710    0.9709      2969\n",
      "\n",
      "[[1723   15]\n",
      " [  71 1160]]\n"
     ]
    }
   ],
   "source": [
    "check_point_files_list = ['roberta_large_qemu_0.976_ep12', 'roberta_large_qemu_0.975_ep14',\n",
    "                          'roberta_large_qemu_0.975_ep13', 'roberta_large_qemu_0.973_ep9',\n",
    "                          'roberta_large_qemu_0.973_ep10', 'roberta_large_qemu_0.972_ep8',\n",
    "                          'roberta_large_qemu_0.972_ep15']\n",
    "print('check_point_files_list:', check_point_files_list)\n",
    "\n",
    "df_test = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "\n",
    "for check_point_file in check_point_files_list:\n",
    "    print(f'\\n#######################################{check_point_file}')\n",
    "    check_point_file = f'{MODEL_SAVE_PATH}/{check_point_file}.pt'\n",
    "    embed_dim = 1024\n",
    "    model = ConvClassifier(embed_dim)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(check_point_file))\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
