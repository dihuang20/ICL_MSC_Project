{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# This source code file refers to:\n",
    "# https://github.com/microsoft/CodeBERT\n",
    "# https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ONLY_TESTING = False\n",
    "DATASET_NAME = 'ffmpeg'\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "LR = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "MODEL_SAVE_PATH: /root/autodl-tmp/finetuned_models/ffmpeg\n",
      "DATASET_PATH: /root/autodl-tmp/output_dataset_1/ffmpeg\n",
      "BERT_CONFIG: microsoft/codebert-base\n",
      "ONLY_TESTING: False\n",
      "DATASET_NAME: ffmpeg\n",
      "BATCH_SIZE: 4\n",
      "EPOCHS: 10\n",
      "LR: 1e-06\n",
      "using device: cuda\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from transformers import RobertaTokenizer, RobertaConfig, RobertaModel\n",
    "import torch.nn.functional as F\n",
    "import shutil\n",
    "\n",
    "# The following randomization refers to: https://github.com/ICL-ml4csec/VulBERTa/blob/main/Finetuning_VulBERTa-MLP.ipynb\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DATASET_ROOT_PATH = '/root/autodl-tmp'\n",
    "DATASET_PATH = f'{DATASET_ROOT_PATH}/output_dataset_1/{DATASET_NAME}'\n",
    "MODEL_SAVE_PATH = f'{DATASET_ROOT_PATH}/finetuned_models/{DATASET_NAME}'\n",
    "\n",
    "BERT_CONFIG = 'microsoft/codebert-base' # microsoft/codebert-base\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('seed:', seed)\n",
    "print('MODEL_SAVE_PATH:', MODEL_SAVE_PATH)\n",
    "print('DATASET_PATH:', DATASET_PATH)\n",
    "print('BERT_CONFIG:', BERT_CONFIG)\n",
    "\n",
    "print('ONLY_TESTING:', ONLY_TESTING)\n",
    "print('DATASET_NAME:', DATASET_NAME)\n",
    "\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "print('EPOCHS:', EPOCHS)\n",
    "print('LR:', LR)\n",
    "\n",
    "print('using device:', device)\n",
    "print('GPU count:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "def remove_file_if_exist(path):\n",
    "    if not path: return\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except:\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "mkdir_if_not_exist(f'{DATASET_ROOT_PATH}/finetuned_models')\n",
    "# remove_file_if_exist(MODEL_SAVE_PATH)\n",
    "mkdir_if_not_exist(MODEL_SAVE_PATH)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(BERT_CONFIG)\n",
    "\n",
    "def tokenize_helper(x):\n",
    "    code_tokens = tokenizer.tokenize(x)\n",
    "    tokens = [tokenizer.cls_token] + code_tokens + [tokenizer.sep_token]\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tokens_ids = torch.tensor(tokens_ids)\n",
    "    \n",
    "    number_to_pad = 512 - len(tokens_ids)\n",
    "    if number_to_pad > 0:\n",
    "        zero_pad = torch.zeros(512 - len(tokens_ids), dtype=int)\n",
    "        tokens_ids = torch.cat((tokens_ids, zero_pad), 0)\n",
    "\n",
    "    return tokens_ids[:512]\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [label for label in df['label']]\n",
    "        self.texts = [tokenize_helper(text) for text in df['commit_patch']]\n",
    "        assert(len(self.labels) == len(self.texts))\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "class CodeBertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(CodeBertClassifier, self).__init__()\n",
    "\n",
    "        self.codebert = RobertaModel.from_pretrained(BERT_CONFIG)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(768, 768)\n",
    "        self.out = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.codebert(x)[1]\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def check_parameters(self):\n",
    "        print('The number of CodeBert parameters:', self.codebert.num_parameters())\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            train_input = train_input.to(device)\n",
    "\n",
    "            output = model(train_input)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                # val_input = val_input.squeeze(1).to(device)\n",
    "                val_input = val_input.to(device)\n",
    "\n",
    "                output = model(val_input)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "        bert_config = 'base'\n",
    "        val_acc = f'{total_acc_val / len(val_data):.3f}'\n",
    "        torch.save(model.state_dict(), f'{MODEL_SAVE_PATH}/codebert_{bert_config}_mlp_eachDropout_{val_acc}_ep{epoch_num + 1}.pt')\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    total_acc_test = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            # test_input = test_input.squeeze(1).to(device)\n",
    "            test_input = test_input.to(device)\n",
    "\n",
    "            output = model(test_input)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            test_label = test_label.data.cpu().numpy()\n",
    "            predic = output.argmax(dim=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, test_label)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    print(report)\n",
    "    print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of CodeBert parameters: 124645632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:41<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.169             | Train Accuracy:  0.573             | Val Loss:  0.168             | Val Accuracy:  0.574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:43<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.166             | Train Accuracy:  0.574             | Val Loss:  0.163             | Val Accuracy:  0.596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:42<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.162             | Train Accuracy:  0.611             | Val Loss:  0.160             | Val Accuracy:  0.613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:42<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.158             | Train Accuracy:  0.626             | Val Loss:  0.159             | Val Accuracy:  0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:42<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.155             | Train Accuracy:  0.647             | Val Loss:  0.158             | Val Accuracy:  0.621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:42<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.149             | Train Accuracy:  0.668             | Val Loss:  0.156             | Val Accuracy:  0.645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:42<00:00, 11.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.143             | Train Accuracy:  0.693             | Val Loss:  0.157             | Val Accuracy:  0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:42<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.134             | Train Accuracy:  0.724             | Val Loss:  0.161             | Val Accuracy:  0.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:42<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.123             | Train Accuracy:  0.760             | Val Loss:  0.167             | Val Accuracy:  0.647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [03:42<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.111             | Train Accuracy:  0.796             | Val Loss:  0.178             | Val Accuracy:  0.645\n",
      "Test Accuracy:  0.645\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.6776    0.7258    0.7009      1995\n",
      "  vulnerable     0.5939    0.5373    0.5642      1489\n",
      "\n",
      "    accuracy                         0.6452      3484\n",
      "   macro avg     0.6357    0.6315    0.6325      3484\n",
      "weighted avg     0.6418    0.6452    0.6424      3484\n",
      "\n",
      "[[1448  547]\n",
      " [ 689  800]]\n"
     ]
    }
   ],
   "source": [
    "if not ONLY_TESTING:\n",
    "    model = CodeBertClassifier()\n",
    "    model.check_parameters()\n",
    "    model.to(device)\n",
    "\n",
    "    df_train = pd.read_json(f'{DATASET_PATH}/train.json')\n",
    "    df_val = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "    df_test = pd.read_json(f'{DATASET_PATH}/test.json')\n",
    "    train(model, df_train, df_val, LR, EPOCHS)\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_files_list: ['codebert_base_mlp_eachDropout_0.648_ep8', 'codebert_base_mlp_eachDropout_0.648_ep7', 'codebert_base_mlp_eachDropout_0.647_ep9', 'codebert_base_mlp_eachDropout_0.645_ep6', 'codebert_base_mlp_eachDropout_0.645_ep10']\n",
      "\n",
      "#######################################codebert_base_mlp_eachDropout_0.648_ep8\n",
      "Test Accuracy:  0.651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.6804    0.7353    0.7068      1995\n",
      "  vulnerable     0.6024    0.5373    0.5680      1489\n",
      "\n",
      "    accuracy                         0.6507      3484\n",
      "   macro avg     0.6414    0.6363    0.6374      3484\n",
      "weighted avg     0.6471    0.6507    0.6475      3484\n",
      "\n",
      "[[1467  528]\n",
      " [ 689  800]]\n",
      "\n",
      "#######################################codebert_base_mlp_eachDropout_0.648_ep7\n",
      "Test Accuracy:  0.647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.6815    0.7198    0.7001      1995\n",
      "  vulnerable     0.5940    0.5494    0.5708      1489\n",
      "\n",
      "    accuracy                         0.6470      3484\n",
      "   macro avg     0.6378    0.6346    0.6355      3484\n",
      "weighted avg     0.6441    0.6470    0.6449      3484\n",
      "\n",
      "[[1436  559]\n",
      " [ 671  818]]\n",
      "\n",
      "#######################################codebert_base_mlp_eachDropout_0.647_ep9\n",
      "Test Accuracy:  0.651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.6826    0.7298    0.7054      1995\n",
      "  vulnerable     0.6010    0.5453    0.5718      1489\n",
      "\n",
      "    accuracy                         0.6510      3484\n",
      "   macro avg     0.6418    0.6376    0.6386      3484\n",
      "weighted avg     0.6477    0.6510    0.6483      3484\n",
      "\n",
      "[[1456  539]\n",
      " [ 677  812]]\n",
      "\n",
      "#######################################codebert_base_mlp_eachDropout_0.645_ep6\n",
      "Test Accuracy:  0.642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.6880    0.6852    0.6866      1995\n",
      "  vulnerable     0.5805    0.5836    0.5820      1489\n",
      "\n",
      "    accuracy                         0.6418      3484\n",
      "   macro avg     0.6342    0.6344    0.6343      3484\n",
      "weighted avg     0.6420    0.6418    0.6419      3484\n",
      "\n",
      "[[1367  628]\n",
      " [ 620  869]]\n",
      "\n",
      "#######################################codebert_base_mlp_eachDropout_0.645_ep10\n",
      "Test Accuracy:  0.647\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.6783    0.7283    0.7024      1995\n",
      "  vulnerable     0.5961    0.5373    0.5652      1489\n",
      "\n",
      "    accuracy                         0.6467      3484\n",
      "   macro avg     0.6372    0.6328    0.6338      3484\n",
      "weighted avg     0.6432    0.6467    0.6438      3484\n",
      "\n",
      "[[1453  542]\n",
      " [ 689  800]]\n"
     ]
    }
   ],
   "source": [
    "check_point_files_list = ['codebert_base_mlp_eachDropout_0.648_ep8', 'codebert_base_mlp_eachDropout_0.648_ep7',\n",
    "                          'codebert_base_mlp_eachDropout_0.647_ep9', 'codebert_base_mlp_eachDropout_0.645_ep6',\n",
    "                          'codebert_base_mlp_eachDropout_0.645_ep10']\n",
    "print('check_point_files_list:', check_point_files_list)\n",
    "\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "df_test = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "\n",
    "for check_point_file in check_point_files_list:\n",
    "    print(f'\\n#######################################{check_point_file}')\n",
    "    check_point_file = f'{MODEL_SAVE_PATH}/{check_point_file}.pt'\n",
    "    model = CodeBertClassifier()\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(check_point_file))\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
