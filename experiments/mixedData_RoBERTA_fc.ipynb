{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# This source code file refers to:\n",
    "# https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n",
    "# https://huggingface.co/docs/transformers/model_doc/roberta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import os\n",
    "import random\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments, RobertaModel\n",
    "\n",
    "# The following randomization refers to: https://github.com/ICL-ml4csec/VulBERTa/blob/main/Finetuning_VulBERTa-MLP.ipynb\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "MODEL_SAVE_PATH = '/root/autodl-tmp/finetuned_models'\n",
    "DATASET_ROOT_PATH = '/root/autodl-tmp'\n",
    "BERT_CONFIG = 'roberta-base' # roberta-base, roberta-large\n",
    "MODEL_NAME = 'fc'\n",
    "labels = {0:0, 1:1}\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "ONLY_TESTING = False\n",
    "\n",
    "DATASET_NAME = 'combined'\n",
    "# DATASET_MASKING = '_masked'\n",
    "DATASET_MASKING = ''\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "LR = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "MODEL_SAVE_PATH: /root/autodl-tmp/finetuned_models/combined\n",
      "DATASET_PATH: /root/autodl-tmp/output_dataset_1/combined\n",
      "BERT_CONFIG: roberta-base\n",
      "ONLY_TESTING: False\n",
      "DATASET_NAME: combined\n",
      "DATASET_MASKING: \n",
      "BATCH_SIZE: 4\n",
      "EPOCHS: 10\n",
      "LR: 1e-06\n",
      "using device: cuda\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(BERT_CONFIG)\n",
    "DATASET_PATH = f'{DATASET_ROOT_PATH}/output_dataset_1/{DATASET_NAME}{DATASET_MASKING}'\n",
    "MODEL_SAVE_PATH = f'{MODEL_SAVE_PATH}/{DATASET_NAME}{DATASET_MASKING}'\n",
    "\n",
    "print('seed:', seed)\n",
    "print('MODEL_SAVE_PATH:', MODEL_SAVE_PATH)\n",
    "print('DATASET_PATH:', DATASET_PATH)\n",
    "print('BERT_CONFIG:', BERT_CONFIG)\n",
    "\n",
    "print('ONLY_TESTING:', ONLY_TESTING)\n",
    "print('DATASET_NAME:', DATASET_NAME)\n",
    "print('DATASET_MASKING:', DATASET_MASKING)\n",
    "\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "print('EPOCHS:', EPOCHS)\n",
    "print('LR:', LR)\n",
    "\n",
    "print('using device:', device)\n",
    "print('GPU count:', torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "mkdir_if_not_exist(MODEL_SAVE_PATH)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['label']]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['commit_message']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = RobertaModel.from_pretrained(BERT_CONFIG)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if BERT_CONFIG == 'roberta-large':\n",
    "            self.linear = nn.Linear(1024, len(labels))\n",
    "        else:\n",
    "            self.linear = nn.Linear(768, len(labels))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        # final_layer = self.relu(linear_output) # IMPO CHANGE\n",
    "        return linear_output\n",
    "\n",
    "    def check_parameters(self):\n",
    "        print('The number of Bert parameters:', self.bert.num_parameters())\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        model.train()\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .4f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .4f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .4f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .4f}')\n",
    "\n",
    "        bert_config = 'large' if BERT_CONFIG == 'roberta-large' else 'base'\n",
    "        val_acc = f'{total_acc_val / len(val_data):.4f}'\n",
    "        torch.save(model.state_dict(), f'{MODEL_SAVE_PATH}/roberta_{bert_config}_{MODEL_NAME}_{val_acc}_ep{epoch_num + 1}.pt')\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    total_acc_test = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            test_label = test_label.data.cpu().numpy()\n",
    "            predic = output.argmax(dim=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, test_label)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .4f}')\n",
    "    print(report)\n",
    "    print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 4709/4709 [06:58<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.1128             | Train Accuracy:  0.7561             | Val Loss:  0.0592             | Val Accuracy:  0.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4709/4709 [06:59<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.0521             | Train Accuracy:  0.9224             | Val Loss:  0.0464             | Val Accuracy:  0.9391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4709/4709 [06:58<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.0371             | Train Accuracy:  0.9493             | Val Loss:  0.0394             | Val Accuracy:  0.9550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4709/4709 [06:58<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.0292             | Train Accuracy:  0.9629             | Val Loss:  0.0384             | Val Accuracy:  0.9569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4709/4709 [06:58<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.0237             | Train Accuracy:  0.9707             | Val Loss:  0.0333             | Val Accuracy:  0.9650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4709/4709 [06:59<00:00, 11.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.0204             | Train Accuracy:  0.9751             | Val Loss:  0.0294             | Val Accuracy:  0.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4709/4709 [06:58<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.0177             | Train Accuracy:  0.9798             | Val Loss:  0.0279             | Val Accuracy:  0.9699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4709/4709 [06:58<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.0144             | Train Accuracy:  0.9833             | Val Loss:  0.0253             | Val Accuracy:  0.9743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4709/4709 [06:59<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.0111             | Train Accuracy:  0.9876             | Val Loss:  0.0250             | Val Accuracy:  0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4709/4709 [06:59<00:00, 11.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.0089             | Train Accuracy:  0.9906             | Val Loss:  0.0233             | Val Accuracy:  0.9767\n"
     ]
    }
   ],
   "source": [
    "if not ONLY_TESTING:\n",
    "    model = BertClassifier()\n",
    "    model.to(device)\n",
    "\n",
    "    df_train = pd.read_json(f'{DATASET_PATH}/train.json')\n",
    "    df_val = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "    train(model, df_train, df_val, LR, EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_files_list: ['roberta_base_fc_0.9767_ep10', 'roberta_base_fc_0.9767_ep9']\n",
      "Testing val dataset:\n",
      "\n",
      "#######################################roberta_base_fc_0.9767_ep10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9766    0.9774    0.9770      2389\n",
      "  vulnerable     0.9767    0.9759    0.9763      2322\n",
      "\n",
      "    accuracy                         0.9767      4711\n",
      "   macro avg     0.9767    0.9766    0.9766      4711\n",
      "weighted avg     0.9767    0.9767    0.9767      4711\n",
      "\n",
      "[[2335   54]\n",
      " [  56 2266]]\n",
      "\n",
      "#######################################roberta_base_fc_0.9767_ep9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.9767\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9746    0.9795    0.9770      2389\n",
      "  vulnerable     0.9788    0.9737    0.9763      2322\n",
      "\n",
      "    accuracy                         0.9767      4711\n",
      "   macro avg     0.9767    0.9766    0.9766      4711\n",
      "weighted avg     0.9767    0.9767    0.9766      4711\n",
      "\n",
      "[[2340   49]\n",
      " [  61 2261]]\n"
     ]
    }
   ],
   "source": [
    "check_point_files_list = ['roberta_base_fc_0.9767_ep10', 'roberta_base_fc_0.9767_ep9']\n",
    "print('check_point_files_list:', check_point_files_list)\n",
    "\n",
    "df_val = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "\n",
    "def test_model(df_dataset):\n",
    "    for check_point_file in check_point_files_list:\n",
    "        print(f'\\n#######################################{check_point_file}')\n",
    "        check_point_file = f'{MODEL_SAVE_PATH}/{check_point_file}.pt'\n",
    "        model = BertClassifier()\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(check_point_file))\n",
    "        evaluate(model, df_dataset)\n",
    "\n",
    "print('Testing val dataset:')\n",
    "test_model(df_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
