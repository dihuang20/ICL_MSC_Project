{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# This source code file refers to:\n",
    "# https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n",
    "# https://huggingface.co/docs/transformers/model_doc/roberta\n",
    "# https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/1_pretrained_vectors.ipynb\n",
    "# https://github.com/dpressel/mead-baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import os\n",
    "import random\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments, RobertaModel\n",
    "\n",
    "# The following randomization refers to: https://github.com/ICL-ml4csec/VulBERTa/blob/main/Finetuning_VulBERTa-MLP.ipynb\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "MODEL_SAVE_PATH = '/root/autodl-tmp/finetuned_models'\n",
    "DATASET_ROOT_PATH = '/root/autodl-tmp'\n",
    "BERT_CONFIG = 'roberta-large' # roberta-base, roberta-large\n",
    "MODEL_NAME = 'cnn'\n",
    "labels = {0:0, 1:1}\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "ONLY_TESTING = False\n",
    "\n",
    "DATASET_NAME = 'qemu'\n",
    "DATASET_MASKING = 'masked_'\n",
    "# DATASET_MASKING = ''\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 15\n",
    "LR = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "MODEL_SAVE_PATH: /root/autodl-tmp/finetuned_models/masked_qemu\n",
      "DATASET_PATH: /root/autodl-tmp/output_dataset_1/masked_qemu\n",
      "BERT_CONFIG: roberta-large\n",
      "ONLY_TESTING: False\n",
      "DATASET_NAME: qemu\n",
      "DATASET_MASKING: masked_\n",
      "BATCH_SIZE: 4\n",
      "EPOCHS: 15\n",
      "LR: 1e-06\n",
      "using device: cuda\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(BERT_CONFIG)\n",
    "DATASET_PATH = f'{DATASET_ROOT_PATH}/output_dataset_1/{DATASET_MASKING}{DATASET_NAME}'\n",
    "MODEL_SAVE_PATH = f'{MODEL_SAVE_PATH}/{DATASET_MASKING}{DATASET_NAME}'\n",
    "\n",
    "print('seed:', seed)\n",
    "print('MODEL_SAVE_PATH:', MODEL_SAVE_PATH)\n",
    "print('DATASET_PATH:', DATASET_PATH)\n",
    "print('BERT_CONFIG:', BERT_CONFIG)\n",
    "\n",
    "print('ONLY_TESTING:', ONLY_TESTING)\n",
    "print('DATASET_NAME:', DATASET_NAME)\n",
    "print('DATASET_MASKING:', DATASET_MASKING)\n",
    "\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "print('EPOCHS:', EPOCHS)\n",
    "print('LR:', LR)\n",
    "\n",
    "print('using device:', device)\n",
    "print('GPU count:', torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "mkdir_if_not_exist(MODEL_SAVE_PATH)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['label']]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['commit_message']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "        bert_config = 'large' if BERT_CONFIG == 'roberta-large' else 'base'\n",
    "        val_acc = f'{total_acc_val / len(val_data):.3f}'\n",
    "        torch.save(model.state_dict(), f'{MODEL_SAVE_PATH}/roberta_{bert_config}_{MODEL_NAME}_{val_acc}_ep{epoch_num + 1}.pt')\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    total_acc_test = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            test_label = test_label.data.cpu().numpy()\n",
    "            predic = output.argmax(dim=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, test_label)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    print(report)\n",
    "    print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ParallelConv(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, filters, dropout=0.5):\n",
    "        super().__init__()\n",
    "        convs = []        \n",
    "        self.output_dims = sum([t[1] for t in filters])\n",
    "        for (filter_length, output_dims) in filters:\n",
    "            pad = filter_length//2\n",
    "            conv = nn.Sequential(\n",
    "                nn.Conv1d(input_dims, output_dims, filter_length, padding=pad),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            convs.append(conv)\n",
    "        # Add the module so its managed correctly\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.conv_drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_bct):\n",
    "        mots = []\n",
    "        for conv in self.convs:\n",
    "            # In Conv1d, data BxCxT, max over time\n",
    "            conv_out = conv(input_bct)\n",
    "            mot, _ = conv_out.max(2)\n",
    "            mots.append(mot)\n",
    "        mots = torch.cat(mots, 1)\n",
    "        return self.conv_drop(mots)\n",
    "\n",
    "class ConvClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dims,\n",
    "                 filters=[(2, 100), (3, 100), (4, 100)],\n",
    "                 dropout=0.5, hidden_units=[]):\n",
    "        super().__init__()\n",
    "        self.bert = RobertaModel.from_pretrained(BERT_CONFIG)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.convs = ParallelConv(embed_dims, filters, dropout)\n",
    "        \n",
    "        input_units = self.convs.output_dims\n",
    "        output_units = self.convs.output_dims\n",
    "        sequence = []\n",
    "        for h in hidden_units:\n",
    "            sequence.append(self.dropout(nn.Linear(input_units, h)))\n",
    "            input_units = h\n",
    "            output_units = h\n",
    "            \n",
    "        sequence.append(nn.Linear(output_units, 2))\n",
    "        self.outputs = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        x, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        embed = self.dropout(x)\n",
    "        embed = embed.transpose(1, 2).contiguous()\n",
    "        hidden = self.convs(embed)\n",
    "        linear = self.outputs(hidden)\n",
    "        return F.log_softmax(linear, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 2227/2227 [09:11<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.199             | Train Accuracy:  0.520             | Val Loss:  0.194             | Val Accuracy:  0.533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:12<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.167             | Train Accuracy:  0.622             | Val Loss:  0.124             | Val Accuracy:  0.727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:11<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.112             | Train Accuracy:  0.760             | Val Loss:  0.109             | Val Accuracy:  0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:12<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.093             | Train Accuracy:  0.795             | Val Loss:  0.103             | Val Accuracy:  0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:12<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.085             | Train Accuracy:  0.800             | Val Loss:  0.118             | Val Accuracy:  0.767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:11<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.081             | Train Accuracy:  0.808             | Val Loss:  0.100             | Val Accuracy:  0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:11<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.077             | Train Accuracy:  0.813             | Val Loss:  0.106             | Val Accuracy:  0.786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:11<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.076             | Train Accuracy:  0.811             | Val Loss:  0.107             | Val Accuracy:  0.773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:12<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.075             | Train Accuracy:  0.811             | Val Loss:  0.106             | Val Accuracy:  0.787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:12<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.073             | Train Accuracy:  0.820             | Val Loss:  0.111             | Val Accuracy:  0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:12<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.072             | Train Accuracy:  0.819             | Val Loss:  0.108             | Val Accuracy:  0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:11<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.072             | Train Accuracy:  0.813             | Val Loss:  0.110             | Val Accuracy:  0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:12<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.071             | Train Accuracy:  0.822             | Val Loss:  0.118             | Val Accuracy:  0.778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:11<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.071             | Train Accuracy:  0.816             | Val Loss:  0.110             | Val Accuracy:  0.785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:11<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.070             | Train Accuracy:  0.820             | Val Loss:  0.113             | Val Accuracy:  0.794\n",
      "Test Accuracy:  0.782\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7871    0.8613    0.8225      1738\n",
      "  vulnerable     0.7741    0.6710    0.7189      1231\n",
      "\n",
      "    accuracy                         0.7824      2969\n",
      "   macro avg     0.7806    0.7662    0.7707      2969\n",
      "weighted avg     0.7817    0.7824    0.7796      2969\n",
      "\n",
      "[[1497  241]\n",
      " [ 405  826]]\n"
     ]
    }
   ],
   "source": [
    "if not ONLY_TESTING:\n",
    "    embed_dim = 1024\n",
    "    model = ConvClassifier(embed_dim)\n",
    "    model.to(device)\n",
    "    \n",
    "    df_train = pd.read_json(f'{DATASET_PATH}/train.json')\n",
    "    df_val = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "    df_test = pd.read_json(f'{DATASET_PATH}/test.json')\n",
    "    train(model, df_train, df_val, LR, EPOCHS)\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_files_list: ['roberta_large_cnn_0.794_ep15(masked_qemu_msgTF)', 'roberta_large_cnn_0.787_ep9', 'roberta_large_cnn_0.786_ep7', 'roberta_large_cnn_0.786_ep6', 'roberta_large_cnn_0.785_ep14', 'roberta_large_cnn_0.784_ep11', 'roberta_large_cnn_0.783_ep10']\n",
      "\n",
      "#######################################roberta_large_cnn_0.794_ep15(masked_qemu_msgTF)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.742\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9593    0.5834    0.7256      1738\n",
      "  vulnerable     0.6213    0.9651    0.7560      1231\n",
      "\n",
      "    accuracy                         0.7417      2969\n",
      "   macro avg     0.7903    0.7742    0.7408      2969\n",
      "weighted avg     0.8192    0.7417    0.7382      2969\n",
      "\n",
      "[[1014  724]\n",
      " [  43 1188]]\n",
      "\n",
      "#######################################roberta_large_cnn_0.787_ep9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.808\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7613    0.9781    0.8562      1738\n",
      "  vulnerable     0.9484    0.5670    0.7097      1231\n",
      "\n",
      "    accuracy                         0.8077      2969\n",
      "   macro avg     0.8548    0.7726    0.7830      2969\n",
      "weighted avg     0.8389    0.8077    0.7955      2969\n",
      "\n",
      "[[1700   38]\n",
      " [ 533  698]]\n",
      "\n",
      "#######################################roberta_large_cnn_0.786_ep7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.810\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7604    0.9862    0.8587      1738\n",
      "  vulnerable     0.9664    0.5613    0.7102      1231\n",
      "\n",
      "    accuracy                         0.8100      2969\n",
      "   macro avg     0.8634    0.7738    0.7844      2969\n",
      "weighted avg     0.8458    0.8100    0.7971      2969\n",
      "\n",
      "[[1714   24]\n",
      " [ 540  691]]\n",
      "\n",
      "#######################################roberta_large_cnn_0.786_ep6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.810\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.7585    0.9902    0.8590      1738\n",
      "  vulnerable     0.9757    0.5548    0.7074      1231\n",
      "\n",
      "    accuracy                         0.8097      2969\n",
      "   macro avg     0.8671    0.7725    0.7832      2969\n",
      "weighted avg     0.8486    0.8097    0.7961      2969\n",
      "\n",
      "[[1721   17]\n",
      " [ 548  683]]\n",
      "\n",
      "#######################################roberta_large_cnn_0.785_ep14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9558    0.5852    0.7259      1738\n",
      "  vulnerable     0.6215    0.9618    0.7551      1231\n",
      "\n",
      "    accuracy                         0.7413      2969\n",
      "   macro avg     0.7887    0.7735    0.7405      2969\n",
      "weighted avg     0.8172    0.7413    0.7380      2969\n",
      "\n",
      "[[1017  721]\n",
      " [  47 1184]]\n",
      "\n",
      "#######################################roberta_large_cnn_0.784_ep11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.741\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9558    0.5846    0.7255      1738\n",
      "  vulnerable     0.6212    0.9618    0.7549      1231\n",
      "\n",
      "    accuracy                         0.7410      2969\n",
      "   macro avg     0.7885    0.7732    0.7402      2969\n",
      "weighted avg     0.8171    0.7410    0.7376      2969\n",
      "\n",
      "[[1016  722]\n",
      " [  47 1184]]\n",
      "\n",
      "#######################################roberta_large_cnn_0.783_ep10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.740\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9390    0.5938    0.7275      1738\n",
      "  vulnerable     0.6225    0.9456    0.7507      1231\n",
      "\n",
      "    accuracy                         0.7396      2969\n",
      "   macro avg     0.7807    0.7697    0.7391      2969\n",
      "weighted avg     0.8078    0.7396    0.7371      2969\n",
      "\n",
      "[[1032  706]\n",
      " [  67 1164]]\n"
     ]
    }
   ],
   "source": [
    "check_point_files_list = ['roberta_large_cnn_0.794_ep15(masked_qemu_msgTF)',\n",
    "                          'roberta_large_cnn_0.787_ep9', 'roberta_large_cnn_0.786_ep7',\n",
    "                          'roberta_large_cnn_0.786_ep6', 'roberta_large_cnn_0.785_ep14',\n",
    "                          'roberta_large_cnn_0.784_ep11', 'roberta_large_cnn_0.783_ep10']\n",
    "print('check_point_files_list:', check_point_files_list)\n",
    "\n",
    "df_test = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "\n",
    "for check_point_file in check_point_files_list:\n",
    "    print(f'\\n#######################################{check_point_file}')\n",
    "    check_point_file = f'{MODEL_SAVE_PATH}/{check_point_file}.pt'\n",
    "    embed_dim = 1024\n",
    "    model = ConvClassifier(embed_dim)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(check_point_file))\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
