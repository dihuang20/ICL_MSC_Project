{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference:\n",
    "# This source code file refers to: https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import os\n",
    "import random\n",
    "\n",
    "# The following randomization refers to: https://github.com/ICL-ml4csec/VulBERTa/blob/main/Finetuning_VulBERTa-MLP.ipynb\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "MODEL_SAVE_PATH = '/root/autodl-tmp/finetuned_models'\n",
    "DATASET_ROOT_PATH = '/root/autodl-tmp'\n",
    "BERT_CONFIG = 'bert-large-cased' # bert-large-cased , bert-base-cased\n",
    "labels = {0:0, 1:1}\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "ONLY_TESTING = False\n",
    "\n",
    "MODEL_NAME = 'fc'\n",
    "DATASET_NAME = 'qemu'\n",
    "# DATASET_MASKING = 'masked_'\n",
    "DATASET_MASKING = ''\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 15\n",
    "LR = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "MODEL_SAVE_PATH: /root/autodl-tmp/finetuned_models/qemu\n",
      "DATASET_PATH: /root/autodl-tmp/output_dataset_1/qemu\n",
      "BERT_CONFIG: bert-large-cased\n",
      "ONLY_TESTING: False\n",
      "DATASET_NAME: qemu\n",
      "DATASET_MASKING: \n",
      "BATCH_SIZE: 4\n",
      "EPOCHS: 15\n",
      "LR: 1e-06\n",
      "using device: cuda\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_CONFIG)\n",
    "DATASET_PATH = f'{DATASET_ROOT_PATH}/output_dataset_1/{DATASET_MASKING}{DATASET_NAME}'\n",
    "MODEL_SAVE_PATH = f'{MODEL_SAVE_PATH}/{DATASET_MASKING}{DATASET_NAME}'\n",
    "\n",
    "print('seed:', seed)\n",
    "print('MODEL_SAVE_PATH:', MODEL_SAVE_PATH)\n",
    "print('DATASET_PATH:', DATASET_PATH)\n",
    "print('BERT_CONFIG:', BERT_CONFIG)\n",
    "\n",
    "print('ONLY_TESTING:', ONLY_TESTING)\n",
    "print('DATASET_NAME:', DATASET_NAME)\n",
    "print('DATASET_MASKING:', DATASET_MASKING)\n",
    "\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "print('EPOCHS:', EPOCHS)\n",
    "print('LR:', LR)\n",
    "\n",
    "print('using device:', device)\n",
    "print('GPU count:', torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "mkdir_if_not_exist(MODEL_SAVE_PATH)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['label']]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['commit_message']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(BERT_CONFIG)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if BERT_CONFIG == 'bert-large-cased':\n",
    "            self.linear = nn.Linear(1024, len(labels))\n",
    "        else:\n",
    "            self.linear = nn.Linear(768, len(labels))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        # final_layer = self.relu(linear_output) # IMPO CHANGE\n",
    "        return linear_output\n",
    "\n",
    "    def check_parameters(self):\n",
    "        print('The number of Bert parameters:', self.bert.num_parameters())\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    # train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True) # IMPO CHANGE\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "        bert_config = 'large' if BERT_CONFIG == 'bert-large-cased' else 'base'\n",
    "        val_acc = f'{total_acc_val / len(val_data):.3f}'\n",
    "        torch.save(model.state_dict(), f'{MODEL_SAVE_PATH}/bert_{bert_config}_{MODEL_NAME}_{val_acc}_ep{epoch_num + 1}.pt')\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    total_acc_test = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            test_label = test_label.data.cpu().numpy()\n",
    "            predic = output.argmax(dim=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, test_label)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    print(report)\n",
    "    print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Bert parameters: 333579264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:02<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.156             | Train Accuracy:  0.657             | Val Loss:  0.101             | Val Accuracy:  0.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:03<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.082             | Train Accuracy:  0.877             | Val Loss:  0.069             | Val Accuracy:  0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:05<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.050             | Train Accuracy:  0.936             | Val Loss:  0.066             | Val Accuracy:  0.903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:04<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.032             | Train Accuracy:  0.966             | Val Loss:  0.060             | Val Accuracy:  0.918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:04<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.018             | Train Accuracy:  0.982             | Val Loss:  0.051             | Val Accuracy:  0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:04<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.013             | Train Accuracy:  0.989             | Val Loss:  0.056             | Val Accuracy:  0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:04<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.008             | Train Accuracy:  0.993             | Val Loss:  0.050             | Val Accuracy:  0.944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 642/2227 [02:37<06:28,  4.08it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 2227/2227 [09:05<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.006             | Train Accuracy:  0.995             | Val Loss:  0.052             | Val Accuracy:  0.952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:03<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.004             | Train Accuracy:  0.995             | Val Loss:  0.057             | Val Accuracy:  0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:04<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.003             | Train Accuracy:  0.996             | Val Loss:  0.055             | Val Accuracy:  0.947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:03<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.003             | Train Accuracy:  0.996             | Val Loss:  0.068             | Val Accuracy:  0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:03<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.003             | Train Accuracy:  0.996             | Val Loss:  0.059             | Val Accuracy:  0.948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:04<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.001             | Train Accuracy:  0.998             | Val Loss:  0.057             | Val Accuracy:  0.959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:05<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.001             | Train Accuracy:  0.998             | Val Loss:  0.060             | Val Accuracy:  0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2227/2227 [09:04<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.001             | Train Accuracy:  0.999             | Val Loss:  0.065             | Val Accuracy:  0.960\n",
      "Test Accuracy:  0.960\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9509    0.9816    0.9660      1738\n",
      "  vulnerable     0.9728    0.9285    0.9501      1231\n",
      "\n",
      "    accuracy                         0.9596      2969\n",
      "   macro avg     0.9619    0.9551    0.9581      2969\n",
      "weighted avg     0.9600    0.9596    0.9594      2969\n",
      "\n",
      "[[1706   32]\n",
      " [  88 1143]]\n"
     ]
    }
   ],
   "source": [
    "if not ONLY_TESTING:\n",
    "    model = BertClassifier()\n",
    "    # model = nn.DataParallel(model) # get stuck when using multiple GPUs on AutoDL\n",
    "    model.check_parameters()\n",
    "    model.to(device)\n",
    "    \n",
    "    df_train = pd.read_json(f'{DATASET_PATH}/train.json')\n",
    "    df_val = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "    df_test = pd.read_json(f'{DATASET_PATH}/test.json')\n",
    "    train(model, df_train, df_val, LR, EPOCHS)\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_files_list: ['bert_large_fc_0.960_ep15', 'bert_large_fc_0.959_ep13', 'bert_large_fc_0.955_ep14']\n",
      "\n",
      "#######################################bert_large_fc_0.960_ep15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.960\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9509    0.9816    0.9660      1738\n",
      "  vulnerable     0.9728    0.9285    0.9501      1231\n",
      "\n",
      "    accuracy                         0.9596      2969\n",
      "   macro avg     0.9619    0.9551    0.9581      2969\n",
      "weighted avg     0.9600    0.9596    0.9594      2969\n",
      "\n",
      "[[1706   32]\n",
      " [  88 1143]]\n",
      "\n",
      "#######################################bert_large_fc_0.959_ep13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.959\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9469    0.9856    0.9659      1738\n",
      "  vulnerable     0.9784    0.9220    0.9494      1231\n",
      "\n",
      "    accuracy                         0.9592      2969\n",
      "   macro avg     0.9627    0.9538    0.9576      2969\n",
      "weighted avg     0.9600    0.9592    0.9590      2969\n",
      "\n",
      "[[1713   25]\n",
      " [  96 1135]]\n",
      "\n",
      "#######################################bert_large_fc_0.955_ep14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.955\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9526    0.9718    0.9621      1738\n",
      "  vulnerable     0.9590    0.9318    0.9452      1231\n",
      "\n",
      "    accuracy                         0.9552      2969\n",
      "   macro avg     0.9558    0.9518    0.9537      2969\n",
      "weighted avg     0.9553    0.9552    0.9551      2969\n",
      "\n",
      "[[1689   49]\n",
      " [  84 1147]]\n"
     ]
    }
   ],
   "source": [
    "check_point_files_list = ['bert_large_fc_0.960_ep15', 'bert_large_fc_0.959_ep13',\n",
    "                          'bert_large_fc_0.955_ep14']\n",
    "print('check_point_files_list:', check_point_files_list)\n",
    "\n",
    "df_test = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "\n",
    "for check_point_file in check_point_files_list:\n",
    "    print(f'\\n#######################################{check_point_file}')\n",
    "    check_point_file = f'{MODEL_SAVE_PATH}/{check_point_file}.pt'\n",
    "    model = BertClassifier()\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(check_point_file))\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
