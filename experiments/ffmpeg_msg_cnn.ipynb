{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# This source code file refers to:\n",
    "# https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/1_pretrained_vectors.ipynb\n",
    "# https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'ffmpeg'\n",
    "MODEL_NAME = 'cnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Tuple\n",
    "import io\n",
    "import re\n",
    "import codecs\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "def write_to_file(text, path, mode='a'): # 'a': append; 'w': overwrite\n",
    "    with open(path, mode) as f:\n",
    "        f.write(text)\n",
    "\n",
    "def mkdir_if_not_exist(directory):\n",
    "    if not directory: return\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "def remove_file_if_exist(path):\n",
    "    if not path: return\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except:\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using', device)\n",
    "\n",
    "# The following randomization refers to: https://github.com/ICL-ml4csec/VulBERTa/blob/main/Finetuning_VulBERTa-MLP.ipynb\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "os.environ['WANDB_MODE'] = 'dryrun'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Models for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelConv(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, filters, dropout=0.5):\n",
    "        super().__init__()\n",
    "        convs = []        \n",
    "        self.output_dims = sum([t[1] for t in filters])\n",
    "        for (filter_length, output_dims) in filters:\n",
    "            pad = filter_length//2\n",
    "            conv = nn.Sequential(\n",
    "                nn.Conv1d(input_dims, output_dims, filter_length, padding=pad),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            convs.append(conv)\n",
    "        # Add the module so its managed correctly\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.conv_drop = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, input_bct):\n",
    "        mots = []\n",
    "        for conv in self.convs:\n",
    "            # In Conv1d, data BxCxT, max over time\n",
    "            conv_out = conv(input_bct)\n",
    "            mot, _ = conv_out.max(2)\n",
    "            mots.append(mot)\n",
    "        mots = torch.cat(mots, 1)\n",
    "        return self.conv_drop(mots)\n",
    "\n",
    "class ConvClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embeddings, num_classes, embed_dims,\n",
    "                 filters=[(2, 100), (3, 100), (4, 100)],\n",
    "                 dropout=0.5, hidden_units=[]):\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.convs = ParallelConv(embed_dims, filters, dropout)\n",
    "        \n",
    "        input_units = self.convs.output_dims\n",
    "        output_units = self.convs.output_dims\n",
    "        sequence = []\n",
    "        for h in hidden_units:\n",
    "            sequence.append(self.dropout(nn.Linear(input_units, h)))\n",
    "            input_units = h\n",
    "            output_units = h\n",
    "            \n",
    "        sequence.append(nn.Linear(output_units, num_classes))\n",
    "        self.outputs = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        one_hots, lengths = inputs\n",
    "        embed = self.dropout(self.embeddings(one_hots))\n",
    "        embed = embed.transpose(1, 2).contiguous()\n",
    "        hidden = self.convs(embed)\n",
    "        linear = self.outputs(hidden)\n",
    "        return F.log_softmax(linear, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConfusionMatrix:\n",
    "    \"\"\"Confusion matrix with metrics\n",
    "\n",
    "    This class accumulates classification output, and tracks it in a confusion matrix.\n",
    "    Metrics are available that use the confusion matrix\n",
    "    \"\"\"\n",
    "    def __init__(self, labels):\n",
    "        \"\"\"Constructor with input labels\n",
    "\n",
    "        :param labels: Either a dictionary (`k=int,v=str`) or an array of labels\n",
    "        \"\"\"\n",
    "        if type(labels) is dict:\n",
    "            self.labels = []\n",
    "            for i in range(len(labels)):\n",
    "                self.labels.append(labels[i])\n",
    "        else:\n",
    "            self.labels = labels\n",
    "        nc = len(self.labels)\n",
    "        self._cm = np.zeros((nc, nc), dtype=np.int)\n",
    "\n",
    "    def add(self, truth, guess):\n",
    "        \"\"\"Add a single value to the confusion matrix based off `truth` and `guess`\n",
    "\n",
    "        :param truth: The real `y` value (or ground truth label)\n",
    "        :param guess: The guess for `y` value (or assertion)\n",
    "        \"\"\"\n",
    "\n",
    "        self._cm[truth, guess] += 1\n",
    "\n",
    "    def __str__(self):\n",
    "        values = []\n",
    "        width = max(8, max(len(x) for x in self.labels) + 1)\n",
    "        for i, label in enumerate([''] + self.labels):\n",
    "            values += [\"{:>{width}}\".format(label, width=width+1)]\n",
    "        values += ['\\n']\n",
    "        for i, label in enumerate(self.labels):\n",
    "            values += [\"{:>{width}}\".format(label, width=width+1)]\n",
    "            for j in range(len(self.labels)):\n",
    "                values += [\"{:{width}d}\".format(self._cm[i, j], width=width + 1)]\n",
    "            values += ['\\n']\n",
    "        values += ['\\n']\n",
    "        return ''.join(values)\n",
    "\n",
    "    def save(self, outfile):\n",
    "        ordered_fieldnames = OrderedDict([(\"labels\", None)] + [(l, None) for l in self.labels])\n",
    "        with open(outfile, 'w') as f:\n",
    "            dw = csv.DictWriter(f, delimiter=',', fieldnames=ordered_fieldnames)\n",
    "            dw.writeheader()\n",
    "            for index, row in enumerate(self._cm):\n",
    "                row_dict = {l: row[i] for i, l in enumerate(self.labels)}\n",
    "                row_dict.update({\"labels\": self.labels[index]})\n",
    "                dw.writerow(row_dict)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the matrix\n",
    "        \"\"\"\n",
    "        self._cm *= 0\n",
    "\n",
    "    def get_correct(self):\n",
    "        \"\"\"Get the diagonals of the confusion matrix\n",
    "\n",
    "        :return: (``int``) Number of correct classifications\n",
    "        \"\"\"\n",
    "        return self._cm.diagonal().sum()\n",
    "\n",
    "    def get_total(self):\n",
    "        \"\"\"Get total classifications\n",
    "\n",
    "        :return: (``int``) total classifications\n",
    "        \"\"\"\n",
    "        return self._cm.sum()\n",
    "\n",
    "    def get_acc(self):\n",
    "        \"\"\"Get the accuracy\n",
    "\n",
    "        :return: (``float``) accuracy\n",
    "        \"\"\"\n",
    "        return float(self.get_correct())/self.get_total()\n",
    "\n",
    "    def get_recall(self):\n",
    "        \"\"\"Get the recall\n",
    "\n",
    "        :return: (``float``) recall\n",
    "        \"\"\"\n",
    "        total = np.sum(self._cm, axis=1)\n",
    "        total = (total == 0) + total\n",
    "        return np.diag(self._cm) / total.astype(float)\n",
    "\n",
    "    def get_support(self):\n",
    "        return np.sum(self._cm, axis=1)\n",
    "\n",
    "    def get_precision(self):\n",
    "        \"\"\"Get the precision\n",
    "        :return: (``float``) precision\n",
    "        \"\"\"\n",
    "\n",
    "        total = np.sum(self._cm, axis=0)\n",
    "        total = (total == 0) + total\n",
    "        return np.diag(self._cm) / total.astype(float)\n",
    "\n",
    "    def get_mean_precision(self):\n",
    "        \"\"\"Get the mean precision across labels\n",
    "\n",
    "        :return: (``float``) mean precision\n",
    "        \"\"\"\n",
    "        return np.mean(self.get_precision())\n",
    "\n",
    "    def get_weighted_precision(self):\n",
    "        return np.sum(self.get_precision() * self.get_support())/float(self.get_total())\n",
    "\n",
    "    def get_mean_recall(self):\n",
    "        \"\"\"Get the mean recall across labels\n",
    "\n",
    "        :return: (``float``) mean recall\n",
    "        \"\"\"\n",
    "        return np.mean(self.get_recall())\n",
    "\n",
    "    def get_weighted_recall(self):\n",
    "        return np.sum(self.get_recall() * self.get_support())/float(self.get_total())\n",
    "\n",
    "    def get_weighted_f(self, beta=1):\n",
    "        return np.sum(self.get_class_f(beta) * self.get_support())/float(self.get_total())\n",
    "\n",
    "    def get_macro_f(self, beta=1):\n",
    "        \"\"\"Get the macro F_b, with adjustable beta (defaulting to F1)\n",
    "\n",
    "        :param beta: (``float``) defaults to 1 (F1)\n",
    "        :return: (``float``) macro F_b\n",
    "        \"\"\"\n",
    "        if beta < 0:\n",
    "            raise Exception('Beta must be greater than 0')\n",
    "        return np.mean(self.get_class_f(beta))\n",
    "\n",
    "    def get_class_f(self, beta=1):\n",
    "        p = self.get_precision()\n",
    "        r = self.get_recall()\n",
    "\n",
    "        b = beta*beta\n",
    "        d = (b * p + r)\n",
    "        d = (d == 0) + d\n",
    "\n",
    "        return (b + 1) * p * r / d\n",
    "\n",
    "    def get_f(self, beta=1):\n",
    "        \"\"\"Get 2 class F_b, with adjustable beta (defaulting to F1)\n",
    "\n",
    "        :param beta: (``float``) defaults to 1 (F1)\n",
    "        :return: (``float``) 2-class F_b\n",
    "        \"\"\"\n",
    "        p = self.get_precision()[1]\n",
    "        r = self.get_recall()[1]\n",
    "        if beta < 0:\n",
    "            raise Exception('Beta must be greater than 0')\n",
    "        d = (beta*beta * p + r)\n",
    "        if d == 0:\n",
    "            return 0\n",
    "        return (beta*beta + 1) * p * r / d\n",
    "\n",
    "    def get_all_metrics(self):\n",
    "        \"\"\"Make a map of metrics suitable for reporting, keyed by metric name\n",
    "\n",
    "        :return: (``dict``) Map of metrics keyed by metric names\n",
    "        \"\"\"\n",
    "        metrics = {'acc': self.get_acc()}\n",
    "        # If 2 class, assume second class is positive AKA 1\n",
    "        if len(self.labels) == 2:\n",
    "            metrics['precision'] = self.get_precision()[1]\n",
    "            metrics['recall'] = self.get_recall()[1]\n",
    "            metrics['f1'] = self.get_f(1)\n",
    "        else:\n",
    "            metrics['mean_precision'] = self.get_mean_precision()\n",
    "            metrics['mean_recall'] = self.get_mean_recall()\n",
    "            metrics['macro_f1'] = self.get_macro_f(1)\n",
    "            metrics['weighted_precision'] = self.get_weighted_precision()\n",
    "            metrics['weighted_recall'] = self.get_weighted_recall()\n",
    "            metrics['weighted_f1'] = self.get_weighted_f(1)\n",
    "        return metrics\n",
    "\n",
    "    def add_batch(self, truth, guess):\n",
    "        \"\"\"Add a batch of data to the confusion matrix\n",
    "\n",
    "        :param truth: The truth tensor\n",
    "        :param guess: The guess tensor\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for truth_i, guess_i in zip(truth, guess):\n",
    "            self.add(truth_i, guess_i)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, optimizer: torch.optim.Optimizer):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def run(self, model, labels, train, loss, batch_size): \n",
    "        model.train()       \n",
    "        train_loader = DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        cm = ConfusionMatrix(labels)\n",
    "\n",
    "        for batch in train_loader:\n",
    "            loss_value, y_pred, y_actual = self.update(model, loss, batch)\n",
    "            _, best = y_pred.max(1)\n",
    "            yt = y_actual.cpu().int().numpy()\n",
    "            yp = best.cpu().int().numpy()\n",
    "            cm.add_batch(yt, yp)\n",
    "\n",
    "        print(cm.get_all_metrics())\n",
    "        return cm\n",
    "    \n",
    "    def update(self, model, loss, batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        x, lengths, y = batch\n",
    "        lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[perm_idx]\n",
    "        y_sorted = y[perm_idx]\n",
    "        y_sorted = y_sorted.to('cuda:0')\n",
    "        inputs = (x_sorted.to('cuda:0'), lengths)\n",
    "        y_pred = model(inputs)\n",
    "        loss_value = loss(y_pred, y_sorted)\n",
    "        loss_value.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss_value.item(), y_pred, y_sorted\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def run(self, model, labels, dataset, batch_size=1):\n",
    "        model.eval()\n",
    "        valid_loader = DataLoader(dataset, batch_size=batch_size)\n",
    "        cm = ConfusionMatrix(labels)\n",
    "        for batch in valid_loader:\n",
    "            y_pred, y_actual = self.inference(model, batch)\n",
    "            _, best = y_pred.max(1)\n",
    "            yt = y_actual.cpu().int().numpy()\n",
    "            yp = best.cpu().int().numpy()\n",
    "            cm.add_batch(yt, yp)\n",
    "        return cm\n",
    "\n",
    "    def inference(self, model, batch):\n",
    "        with torch.no_grad():\n",
    "            x, lengths, y = batch\n",
    "            lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "            x_sorted = x[perm_idx]\n",
    "            y_sorted = y[perm_idx]\n",
    "            y_sorted = y_sorted.to('cuda:0')\n",
    "            inputs = (x_sorted.to('cuda:0'), lengths)\n",
    "            y_pred = model(inputs)\n",
    "            return y_pred, y_sorted\n",
    "\n",
    "def fit(model, labels, optimizer, loss, epochs, batch_size, train, valid, test):\n",
    "\n",
    "    trainer = Trainer(optimizer)\n",
    "    evaluator = Evaluator()\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print('EPOCH {}'.format(epoch + 1))\n",
    "        print('=================================')\n",
    "        print('Training Results')\n",
    "        cm = trainer.run(model, labels, train, loss, batch_size)\n",
    "        print('Validation Results')\n",
    "        cm = evaluator.run(model, labels, valid)\n",
    "        print(cm.get_all_metrics())\n",
    "\n",
    "        # if cm.get_f() > best_f1:\n",
    "        #     print('New best model {:.2f}'.format(cm.get_f()))\n",
    "        #     best_f1 = cm.get_f()\n",
    "        #     torch.save(model.state_dict(), f'/root/autodl-tmp/finetuned_models/{MODEL_NAME}_{DATASET_NAME}_checkpoint.pth')\n",
    "\n",
    "        if cm.get_acc() > best_acc:\n",
    "            print('New best model {:.2f}'.format(cm.get_acc()))\n",
    "            best_acc = cm.get_acc()\n",
    "            torch.save(model.state_dict(), f'/root/autodl-tmp/finetuned_models/{MODEL_NAME}_{DATASET_NAME}_checkpoint.pth')\n",
    "\n",
    "    if test:\n",
    "        model.load_state_dict(torch.load(f'/root/autodl-tmp/finetuned_models/{MODEL_NAME}_{DATASET_NAME}_checkpoint.pth'))\n",
    "        cm = evaluator.run(model, labels, test)\n",
    "        print('Final result')\n",
    "        print(cm.get_all_metrics())\n",
    "    return cm.get_acc()\n",
    "\n",
    "def whitespace_tokenizer(words: str) -> List[str]:\n",
    "    return words.split() \n",
    "\n",
    "def sst2_tokenizer(words: str) -> List[str]:\n",
    "    REPLACE = { \"'s\": \" 's \",\n",
    "                \"'ve\": \" 've \",\n",
    "                \"n't\": \" n't \",\n",
    "                \"'re\": \" 're \",\n",
    "                \"'d\": \" 'd \",\n",
    "                \"'ll\": \" 'll \",\n",
    "                \",\": \" , \",\n",
    "                \"!\": \" ! \",\n",
    "                }\n",
    "    words = words.lower()\n",
    "    words = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", words)\n",
    "    for k, v in REPLACE.items():\n",
    "            words = words.replace(k, v)\n",
    "    return [w.strip() for w in words.split()]\n",
    "\n",
    "\n",
    "class Reader:\n",
    "\n",
    "    def __init__(self, files, lowercase=True, min_freq=0,\n",
    "                 tokenizer=sst2_tokenizer, vectorizer=None):\n",
    "        self.lowercase = lowercase\n",
    "        self.tokenizer = tokenizer\n",
    "        build_vocab = vectorizer is None\n",
    "        self.vectorizer = vectorizer if vectorizer else self._vectorizer\n",
    "        x = Counter()\n",
    "        y = Counter()\n",
    "        for file_name in files:\n",
    "            if file_name is None:\n",
    "                continue\n",
    "            with codecs.open(file_name, encoding='utf-8', mode='r') as f:\n",
    "                for line in f:\n",
    "                    words = line.split()\n",
    "                    y.update(words[0])\n",
    "\n",
    "                    if build_vocab:\n",
    "                        words = self.tokenizer(' '.join(words[1:]))\n",
    "                        words = words if not self.lowercase else [w.lower() for w in words]\n",
    "                        x.update(words)\n",
    "        self.labels = list(y.keys())\n",
    "\n",
    "        if build_vocab:\n",
    "            x = dict(filter(lambda cnt: cnt[1] >= min_freq, x.items()))\n",
    "            alpha = list(x.keys())\n",
    "            alpha.sort()\n",
    "            self.vocab = {w: i+1 for i, w in enumerate(alpha)}\n",
    "            self.vocab['[PAD]'] = 0\n",
    "\n",
    "        self.labels.sort()\n",
    "\n",
    "    def _vectorizer(self, words: List[str]) -> List[int]:\n",
    "        return [self.vocab.get(w, 0) for w in words]\n",
    "\n",
    "    def load(self, filename: str) -> TensorDataset:\n",
    "        label2index = {l: i for i, l in enumerate(self.labels)}\n",
    "        xs = []\n",
    "        lengths = []\n",
    "        ys = []\n",
    "        with codecs.open(filename, encoding='utf-8', mode='r') as f:\n",
    "            for line in f:\n",
    "                words = line.split()\n",
    "                ys.append(label2index[words[0]])\n",
    "                words = self.tokenizer(' '.join(words[1:]))\n",
    "                words = words if not self.lowercase else [w.lower() for w in words]\n",
    "                vec = self.vectorizer(words)\n",
    "                lengths.append(len(vec))\n",
    "                xs.append(torch.tensor(vec, dtype=torch.long))\n",
    "        x_tensor = torch.nn.utils.rnn.pad_sequence(xs, batch_first=True)\n",
    "        lengths_tensor = torch.tensor(lengths, dtype=torch.long)\n",
    "        y_tensor = torch.tensor(ys, dtype=torch.long)\n",
    "        return TensorDataset(x_tensor, lengths_tensor, y_tensor)\n",
    "\n",
    "def init_embeddings(vocab_size, embed_dim, unif):\n",
    "    return np.random.uniform(-unif, unif, (vocab_size, embed_dim))\n",
    "    \n",
    "\n",
    "class EmbeddingsReader:\n",
    "\n",
    "    @staticmethod\n",
    "    def from_text(filename, vocab, unif=0.25):\n",
    "        \n",
    "        with io.open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                line = line.rstrip(\"\\n \")\n",
    "                values = line.split(\" \")\n",
    "\n",
    "                if i == 0:\n",
    "                    # fastText style\n",
    "                    if len(values) == 2:\n",
    "                        weight = init_embeddings(len(vocab), values[1], unif)\n",
    "                        continue\n",
    "                    # glove style\n",
    "                    else:\n",
    "                        weight = init_embeddings(len(vocab), len(values[1:]), unif)\n",
    "                word = values[0]\n",
    "                if word in vocab:\n",
    "                    vec = np.asarray(values[1:], dtype=np.float32)\n",
    "                    weight[vocab[word]] = vec\n",
    "        if '[PAD]' in vocab:\n",
    "            weight[vocab['[PAD]']] = 0.0\n",
    "        \n",
    "        embeddings = nn.Embedding(weight.shape[0], weight.shape[1])\n",
    "        embeddings.weight = nn.Parameter(torch.from_numpy(weight).float())\n",
    "        return embeddings, weight.shape[1]\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_binary(filename, vocab, unif=0.25):\n",
    "        def read_word(f):\n",
    "\n",
    "            s = bytearray()\n",
    "            ch = f.read(1)\n",
    "\n",
    "            while ch != b' ':\n",
    "                s.extend(ch)\n",
    "                ch = f.read(1)\n",
    "            s = s.decode('utf-8')\n",
    "            # Only strip out normal space and \\n not other spaces which are words.\n",
    "            return s.strip(' \\n')\n",
    "\n",
    "        vocab_size = len(vocab)\n",
    "        with io.open(filename, \"rb\") as f:\n",
    "            header = f.readline()\n",
    "            file_vocab_size, embed_dim = map(int, header.split())\n",
    "            weight = init_embeddings(len(vocab), embed_dim, unif)\n",
    "            if '[PAD]' in vocab:\n",
    "                weight[vocab['[PAD]']] = 0.0\n",
    "            width = 4 * embed_dim\n",
    "            for i in range(file_vocab_size):\n",
    "                word = read_word(f)\n",
    "                raw = f.read(width)\n",
    "                if word in vocab:\n",
    "                    vec = np.fromstring(raw, dtype=np.float32)\n",
    "                    weight[vocab[word]] = vec\n",
    "        embeddings = nn.Embedding(weight.shape[0], weight.shape[1])\n",
    "        embeddings.weight = nn.Parameter(torch.from_numpy(weight).float())\n",
    "        return embeddings, embed_dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_txt(in_file_name, out_file_name):\n",
    "    df_data = pd.read_json(in_file_name)\n",
    "    for _, row in df_data.iterrows():\n",
    "        label = row['label']\n",
    "        commit_message = row['commit_message']\n",
    "        content = f'{int(label)} {commit_message}\\n'\n",
    "        write_to_file(content, out_file_name)\n",
    "\n",
    "in_train_file = f'/root/autodl-tmp/output_dataset_1/{DATASET_NAME}/train.json'      \n",
    "in_val_file = f'/root/autodl-tmp/output_dataset_1/{DATASET_NAME}/val.json'      \n",
    "in_test_file = f'/root/autodl-tmp/output_dataset_1/{DATASET_NAME}/test.json'  \n",
    "\n",
    "out_train_file = f'/root/autodl-tmp/output_dataset_1/{DATASET_NAME}/train.txt'      \n",
    "out_val_file = f'/root/autodl-tmp/output_dataset_1/{DATASET_NAME}/val.txt'      \n",
    "out_test_file = f'/root/autodl-tmp/output_dataset_1/{DATASET_NAME}/test.txt'\n",
    "\n",
    "remove_file_if_exist(out_train_file)\n",
    "remove_file_if_exist(out_val_file)\n",
    "remove_file_if_exist(out_test_file)\n",
    "\n",
    "convert_json_to_txt(in_train_file, out_train_file)\n",
    "convert_json_to_txt(in_val_file, out_val_file)\n",
    "convert_json_to_txt(in_val_file, out_test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 7908602 parameters\n",
      "EPOCH 1\n",
      "=================================\n",
      "Training Results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3488/3026956255.py:19: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  self._cm = np.zeros((nc, nc), dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc': 0.6789508949937781, 'precision': 0.6250281341435967, 'recall': 0.6219484882418813, 'f1': 0.6234845083071398}\n",
      "Validation Results\n",
      "{'acc': 0.8498851894374282, 'precision': 0.8072519083969466, 'recall': 0.8522498321020819, 'f1': 0.829140803658935}\n",
      "New best model 0.85\n",
      "EPOCH 2\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.7604096869914808, 'precision': 0.7200538358008075, 'recall': 0.7189249720044792, 'f1': 0.7194889611117337}\n",
      "Validation Results\n",
      "{'acc': 0.8587830080367393, 'precision': 0.9882468168462292, 'recall': 0.6776359973136333, 'f1': 0.80398406374502}\n",
      "New best model 0.86\n",
      "EPOCH 3\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.7910404900928496, 'precision': 0.7565197841726619, 'recall': 0.7536394176931691, 'f1': 0.7550768540334344}\n",
      "Validation Results\n",
      "{'acc': 0.8912169919632607, 'precision': 0.9859894921190894, 'recall': 0.7562122229684352, 'f1': 0.8559483086278982}\n",
      "New best model 0.89\n",
      "EPOCH 4\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8185124916243898, 'precision': 0.790413746326023, 'recall': 0.7829787234042553, 'f1': 0.7866786678667866}\n",
      "Validation Results\n",
      "{'acc': 0.9009758897818599, 'precision': 0.9905660377358491, 'recall': 0.7756883814640698, 'f1': 0.8700564971751412}\n",
      "New best model 0.90\n",
      "EPOCH 5\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8393797262371973, 'precision': 0.8156285390713477, 'recall': 0.806494960806271, 'f1': 0.8110360360360361}\n",
      "Validation Results\n",
      "{'acc': 0.9293915040183697, 'precision': 0.9506889050036258, 'recall': 0.880456682337139, 'f1': 0.9142259414225942}\n",
      "New best model 0.93\n",
      "EPOCH 6\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8389968411984302, 'precision': 0.8112279132185194, 'recall': 0.8123180291153416, 'f1': 0.81177260519248}\n",
      "Validation Results\n",
      "{'acc': 0.9239380022962113, 'precision': 0.9872611464968153, 'recall': 0.8327736736064473, 'f1': 0.9034608378870673}\n",
      "EPOCH 7\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.847707475830382, 'precision': 0.8219086021505376, 'recall': 0.8217245240761478, 'f1': 0.8218165528054654}\n",
      "Validation Results\n",
      "{'acc': 0.898105625717566, 'precision': 0.9964973730297724, 'recall': 0.7642713230355943, 'f1': 0.8650703154694032}\n",
      "EPOCH 8\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8587154206949363, 'precision': 0.8357672433161087, 'recall': 0.8331466965285554, 'f1': 0.8344549125168238}\n",
      "Validation Results\n",
      "{'acc': 0.9199196326061998, 'precision': 0.991869918699187, 'recall': 0.8193418401611821, 'f1': 0.8973887458624495}\n",
      "EPOCH 9\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8738393797262372, 'precision': 0.8549515001127904, 'recall': 0.8488241881298992, 'f1': 0.8518768262530905}\n",
      "Validation Results\n",
      "{'acc': 0.9245120551090701, 'precision': 0.9927652733118971, 'recall': 0.8294157152451309, 'f1': 0.9037687522868642}\n",
      "EPOCH 10\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8692447592610318, 'precision': 0.84859392575928, 'recall': 0.8447928331466965, 'f1': 0.84668911335578}\n",
      "Validation Results\n",
      "{'acc': 0.9316877152698049, 'precision': 0.9905882352941177, 'recall': 0.8482202820685023, 'f1': 0.9138929088277857}\n",
      "New best model 0.93\n",
      "EPOCH 11\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8849430458504834, 'precision': 0.8663822142375927, 'recall': 0.864053751399776, 'f1': 0.8652164162368244}\n",
      "Validation Results\n",
      "{'acc': 0.9291044776119403, 'precision': 0.990521327014218, 'recall': 0.842175957018133, 'f1': 0.9103448275862069}\n",
      "EPOCH 12\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8838901119938738, 'precision': 0.8663812528165841, 'recall': 0.8611422172452408, 'f1': 0.8637537908570145}\n",
      "Validation Results\n",
      "{'acc': 0.9247990815154994, 'precision': 0.9935639581657281, 'recall': 0.8294157152451309, 'f1': 0.904099560761347}\n",
      "EPOCH 13\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8879104049009285, 'precision': 0.870945945945946, 'recall': 0.8660694288913774, 'f1': 0.86850084222347}\n",
      "Validation Results\n",
      "{'acc': 0.9448909299655568, 'precision': 0.9778924097273397, 'recall': 0.8912021490933513, 'f1': 0.932536893886156}\n",
      "New best model 0.94\n",
      "EPOCH 14\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8993969560639418, 'precision': 0.8832510103277953, 'recall': 0.8810750279955207, 'f1': 0.8821616773180849}\n",
      "Validation Results\n",
      "{'acc': 0.9322617680826636, 'precision': 0.990602975724354, 'recall': 0.8495634654130289, 'f1': 0.9146782357194504}\n",
      "EPOCH 15\n",
      "=================================\n",
      "Training Results\n",
      "{'acc': 0.8998755623624007, 'precision': 0.886677222347885, 'recall': 0.8779395296752519, 'f1': 0.8822867431915373}\n",
      "Validation Results\n",
      "{'acc': 0.9336969001148105, 'precision': 0.9883540372670807, 'recall': 0.854936198791135, 'f1': 0.9168167086784299}\n",
      "Final result\n",
      "{'acc': 0.9448909299655568, 'precision': 0.9778924097273397, 'recall': 0.8912021490933513, 'f1': 0.932536893886156}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9448909299655568"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE = f'/root/autodl-tmp/output_dataset_1/{DATASET_NAME}'\n",
    "TRAIN = os.path.join(BASE, 'train.txt')\n",
    "VALID = os.path.join(BASE, 'val.txt')\n",
    "TEST = os.path.join(BASE, 'test.txt')\n",
    "PRETRAINED_EMBEDDINGS_FILE = '/root/autodl-tmp/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "r = Reader((TRAIN, VALID, TEST,))\n",
    "train = r.load(TRAIN)\n",
    "valid = r.load(VALID)\n",
    "test = r.load(TEST)\n",
    "\n",
    "embed_dim = 300\n",
    "embeddings = nn.Embedding(len(r.vocab), embed_dim)\n",
    "# embeddings, embed_dim = EmbeddingsReader.from_binary(PRETRAINED_EMBEDDINGS_FILE, r.vocab)\n",
    "model  = ConvClassifier(embeddings, len(r.labels), embed_dim)\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model has {num_params} parameters\") \n",
    "\n",
    "model.to('cuda:0')\n",
    "loss = torch.nn.NLLLoss()\n",
    "loss = loss.to('cuda:0')\n",
    "\n",
    "learnable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adadelta(learnable_params, lr=1.0)\n",
    "\n",
    "fit(model, r.labels, optimizer, loss, 15, 50, train, valid, test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Reliability Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "len(r.vocab): 25459\n",
      "r.labels: ['0', '1']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3484/3484 [00:04<00:00, 698.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9238    0.9850    0.9534      1995\n",
      "  vulnerable     0.9779    0.8912    0.9325      1489\n",
      "\n",
      "    accuracy                         0.9449      3484\n",
      "   macro avg     0.9509    0.9381    0.9430      3484\n",
      "weighted avg     0.9469    0.9449    0.9445      3484\n",
      "\n",
      "[[1965   30]\n",
      " [ 162 1327]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('using', device)\n",
    "print('len(r.vocab):', len(r.vocab))\n",
    "print('r.labels:', r.labels)\n",
    "\n",
    "BASE = f'/root/autodl-tmp/output_dataset_1/{DATASET_NAME}'\n",
    "TRAIN = os.path.join(BASE, 'train.txt')\n",
    "VALID = os.path.join(BASE, 'val.txt')\n",
    "TEST = os.path.join(BASE, 'test.txt')\n",
    "\n",
    "r = Reader((TRAIN, VALID, TEST,))\n",
    "train = r.load(TRAIN)\n",
    "valid = r.load(VALID)\n",
    "test = r.load(TEST)\n",
    "\n",
    "embed_dim = 300\n",
    "embeddings = nn.Embedding(len(r.vocab), embed_dim)\n",
    "model  = ConvClassifier(embeddings, len(r.labels), embed_dim)\n",
    "model.load_state_dict(torch.load(f'/root/autodl-tmp/finetuned_models/{MODEL_NAME}_{DATASET_NAME}_checkpoint.pth'))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "total_acc_test = 0\n",
    "predict_all = np.array([], dtype=int)\n",
    "labels_all = np.array([], dtype=int)\n",
    "valid_loader = DataLoader(valid, batch_size=1)\n",
    "\n",
    "for batch in tqdm(valid_loader):\n",
    "    with torch.no_grad():\n",
    "        x, lengths, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model((x, lengths))\n",
    "        acc = (y_pred.argmax(dim=1) == y).sum().item()\n",
    "        total_acc_test += acc\n",
    "        \n",
    "        y = y.data.cpu().numpy()\n",
    "        predic = y_pred.argmax(dim=1).data.cpu().numpy()\n",
    "        labels_all = np.append(labels_all, y)\n",
    "        predict_all = np.append(predict_all, predic)\n",
    "\n",
    "report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "# print(f'Test Accuracy: {total_acc_test / len(valid): .3f}')\n",
    "print(report)\n",
    "print(confusion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
