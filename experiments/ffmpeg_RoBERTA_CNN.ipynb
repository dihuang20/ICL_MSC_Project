{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# References:\n",
    "# This source code file refers to:\n",
    "# https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n",
    "# https://huggingface.co/docs/transformers/model_doc/roberta\n",
    "# https://colab.research.google.com/github/dpressel/dlss-tutorial/blob/master/1_pretrained_vectors.ipynb\n",
    "# https://github.com/dpressel/mead-baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import os\n",
    "import random\n",
    "\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments, RobertaModel\n",
    "\n",
    "# The following randomization refers to: https://github.com/ICL-ml4csec/VulBERTa/blob/main/Finetuning_VulBERTa-MLP.ipynb\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "MODEL_SAVE_PATH = '/root/autodl-tmp/finetuned_models'\n",
    "DATASET_ROOT_PATH = '/root/autodl-tmp'\n",
    "BERT_CONFIG = 'roberta-large' # roberta-base, roberta-large\n",
    "MODEL_NAME = 'cnn'\n",
    "labels = {0:0, 1:1}\n",
    "\n",
    "# -----------------------------------------------\n",
    "\n",
    "ONLY_TESTING = False\n",
    "\n",
    "DATASET_NAME = 'ffmpeg'\n",
    "# DATASET_MASKING = '_masked'\n",
    "DATASET_MASKING = ''\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 15\n",
    "LR = 1e-6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed: 42\n",
      "MODEL_SAVE_PATH: /root/autodl-tmp/finetuned_models/ffmpeg\n",
      "DATASET_PATH: /root/autodl-tmp/output_dataset_1/ffmpeg\n",
      "BERT_CONFIG: roberta-large\n",
      "ONLY_TESTING: False\n",
      "DATASET_NAME: ffmpeg\n",
      "DATASET_MASKING: \n",
      "BATCH_SIZE: 4\n",
      "EPOCHS: 15\n",
      "LR: 1e-06\n",
      "using device: cuda\n",
      "GPU count: 1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(BERT_CONFIG)\n",
    "DATASET_PATH = f'{DATASET_ROOT_PATH}/output_dataset_1/{DATASET_NAME}{DATASET_MASKING}'\n",
    "MODEL_SAVE_PATH = f'{MODEL_SAVE_PATH}/{DATASET_NAME}{DATASET_MASKING}'\n",
    "\n",
    "print('seed:', seed)\n",
    "print('MODEL_SAVE_PATH:', MODEL_SAVE_PATH)\n",
    "print('DATASET_PATH:', DATASET_PATH)\n",
    "print('BERT_CONFIG:', BERT_CONFIG)\n",
    "\n",
    "print('ONLY_TESTING:', ONLY_TESTING)\n",
    "print('DATASET_NAME:', DATASET_NAME)\n",
    "print('DATASET_MASKING:', DATASET_MASKING)\n",
    "\n",
    "print('BATCH_SIZE:', BATCH_SIZE)\n",
    "print('EPOCHS:', EPOCHS)\n",
    "print('LR:', LR)\n",
    "\n",
    "print('using device:', device)\n",
    "print('GPU count:', torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir_if_not_exist(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.mkdir(directory)\n",
    "\n",
    "mkdir_if_not_exist(MODEL_SAVE_PATH)\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.labels = [labels[label] for label in df['label']]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['commit_message']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_texts, batch_y\n",
    "\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_input, val_label in val_dataloader:\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "        \n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "            | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "            | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "\n",
    "        bert_config = 'large' if BERT_CONFIG == 'roberta-large' else 'base'\n",
    "        val_acc = f'{total_acc_val / len(val_data):.3f}'\n",
    "        torch.save(model.state_dict(), f'{MODEL_SAVE_PATH}/roberta_{bert_config}_{MODEL_NAME}_{val_acc}_ep{epoch_num + 1}.pt')\n",
    "\n",
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    total_acc_test = 0\n",
    "    predict_all = np.array([], dtype=int)\n",
    "    labels_all = np.array([], dtype=int)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "\n",
    "            test_label = test_label.data.cpu().numpy()\n",
    "            predic = output.argmax(dim=1).data.cpu().numpy()\n",
    "            labels_all = np.append(labels_all, test_label)\n",
    "            predict_all = np.append(predict_all, predic)\n",
    "\n",
    "    report = metrics.classification_report(labels_all, predict_all, target_names=['benign', 'vulnerable'], digits=4)\n",
    "    confusion = metrics.confusion_matrix(labels_all, predict_all)\n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    print(report)\n",
    "    print(confusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ParallelConv(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, filters, dropout=0.5):\n",
    "        super().__init__()\n",
    "        convs = []        \n",
    "        self.output_dims = sum([t[1] for t in filters])\n",
    "        for (filter_length, output_dims) in filters:\n",
    "            pad = filter_length//2\n",
    "            conv = nn.Sequential(\n",
    "                nn.Conv1d(input_dims, output_dims, filter_length, padding=pad),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            convs.append(conv)\n",
    "        # Add the module so its managed correctly\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        self.conv_drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_bct):\n",
    "        mots = []\n",
    "        for conv in self.convs:\n",
    "            # In Conv1d, data BxCxT, max over time\n",
    "            conv_out = conv(input_bct)\n",
    "            mot, _ = conv_out.max(2)\n",
    "            mots.append(mot)\n",
    "        mots = torch.cat(mots, 1)\n",
    "        return self.conv_drop(mots)\n",
    "\n",
    "class ConvClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dims,\n",
    "                 filters=[(2, 100), (3, 100), (4, 100)],\n",
    "                 dropout=0.5, hidden_units=[]):\n",
    "        super().__init__()\n",
    "        self.bert = RobertaModel.from_pretrained(BERT_CONFIG)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.convs = ParallelConv(embed_dims, filters, dropout)\n",
    "        \n",
    "        input_units = self.convs.output_dims\n",
    "        output_units = self.convs.output_dims\n",
    "        sequence = []\n",
    "        for h in hidden_units:\n",
    "            sequence.append(self.dropout(nn.Linear(input_units, h)))\n",
    "            input_units = h\n",
    "            output_units = h\n",
    "            \n",
    "        sequence.append(nn.Linear(output_units, 2))\n",
    "        self.outputs = nn.Sequential(*sequence)\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        x, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        embed = self.dropout(x)\n",
    "        embed = embed.transpose(1, 2).contiguous()\n",
    "        hidden = self.convs(embed)\n",
    "        linear = self.outputs(hidden)\n",
    "        return F.log_softmax(linear, dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 2612/2612 [10:45<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.139             | Train Accuracy:  0.699             | Val Loss:  0.050             | Val Accuracy:  0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:45<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.032             | Train Accuracy:  0.958             | Val Loss:  0.027             | Val Accuracy:  0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:45<00:00,  4.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.018             | Train Accuracy:  0.980             | Val Loss:  0.026             | Val Accuracy:  0.974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:43<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.011             | Train Accuracy:  0.987             | Val Loss:  0.021             | Val Accuracy:  0.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:43<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.006             | Train Accuracy:  0.995             | Val Loss:  0.022             | Val Accuracy:  0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:48<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.003             | Train Accuracy:  0.997             | Val Loss:  0.025             | Val Accuracy:  0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:49<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.002             | Train Accuracy:  0.997             | Val Loss:  0.025             | Val Accuracy:  0.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:48<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.001             | Train Accuracy:  0.999             | Val Loss:  0.032             | Val Accuracy:  0.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:48<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.001             | Train Accuracy:  0.999             | Val Loss:  0.029             | Val Accuracy:  0.985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:49<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.001             | Train Accuracy:  0.999             | Val Loss:  0.021             | Val Accuracy:  0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:49<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 11 | Train Loss:  0.001             | Train Accuracy:  0.999             | Val Loss:  0.030             | Val Accuracy:  0.985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:49<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 12 | Train Loss:  0.000             | Train Accuracy:  1.000             | Val Loss:  0.029             | Val Accuracy:  0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:49<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 13 | Train Loss:  0.001             | Train Accuracy:  0.999             | Val Loss:  0.029             | Val Accuracy:  0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:48<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 14 | Train Loss:  0.001             | Train Accuracy:  0.999             | Val Loss:  0.028             | Val Accuracy:  0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2612/2612 [10:49<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 15 | Train Loss:  0.000             | Train Accuracy:  1.000             | Val Loss:  0.028             | Val Accuracy:  0.986\n",
      "Test Accuracy:  0.985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9817    0.9930    0.9873      1995\n",
      "  vulnerable     0.9905    0.9752    0.9827      1489\n",
      "\n",
      "    accuracy                         0.9854      3484\n",
      "   macro avg     0.9861    0.9841    0.9850      3484\n",
      "weighted avg     0.9854    0.9854    0.9853      3484\n",
      "\n",
      "[[1981   14]\n",
      " [  37 1452]]\n"
     ]
    }
   ],
   "source": [
    "if not ONLY_TESTING:\n",
    "    embed_dim = 1024\n",
    "    model = ConvClassifier(embed_dim)\n",
    "    model.to(device)\n",
    "    \n",
    "    df_train = pd.read_json(f'{DATASET_PATH}/train.json')\n",
    "    df_val = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "    df_test = pd.read_json(f'{DATASET_PATH}/test.json')\n",
    "    train(model, df_train, df_val, LR, EPOCHS)\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_point_files_list: ['roberta_large_cnn_0.986_ep15', 'roberta_large_cnn_0.985_ep9', 'roberta_large_cnn_0.985_ep11', 'roberta_large_cnn_0.984_ep10']\n",
      "\n",
      "#######################################roberta_large_cnn_0.986_ep15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9817    0.9930    0.9873      1995\n",
      "  vulnerable     0.9905    0.9752    0.9827      1489\n",
      "\n",
      "    accuracy                         0.9854      3484\n",
      "   macro avg     0.9861    0.9841    0.9850      3484\n",
      "weighted avg     0.9854    0.9854    0.9853      3484\n",
      "\n",
      "[[1981   14]\n",
      " [  37 1452]]\n",
      "\n",
      "#######################################roberta_large_cnn_0.985_ep9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.984\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9802    0.9915    0.9858      1995\n",
      "  vulnerable     0.9884    0.9731    0.9807      1489\n",
      "\n",
      "    accuracy                         0.9836      3484\n",
      "   macro avg     0.9843    0.9823    0.9833      3484\n",
      "weighted avg     0.9837    0.9836    0.9836      3484\n",
      "\n",
      "[[1978   17]\n",
      " [  40 1449]]\n",
      "\n",
      "#######################################roberta_large_cnn_0.985_ep11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.984\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9835    0.9880    0.9857      1995\n",
      "  vulnerable     0.9838    0.9778    0.9808      1489\n",
      "\n",
      "    accuracy                         0.9836      3484\n",
      "   macro avg     0.9837    0.9829    0.9833      3484\n",
      "weighted avg     0.9836    0.9836    0.9836      3484\n",
      "\n",
      "[[1971   24]\n",
      " [  33 1456]]\n",
      "\n",
      "#######################################roberta_large_cnn_0.984_ep10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.985\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign     0.9836    0.9895    0.9865      1995\n",
      "  vulnerable     0.9858    0.9778    0.9818      1489\n",
      "\n",
      "    accuracy                         0.9845      3484\n",
      "   macro avg     0.9847    0.9837    0.9842      3484\n",
      "weighted avg     0.9845    0.9845    0.9845      3484\n",
      "\n",
      "[[1974   21]\n",
      " [  33 1456]]\n"
     ]
    }
   ],
   "source": [
    "check_point_files_list = ['roberta_large_cnn_0.986_ep15', 'roberta_large_cnn_0.985_ep9',\n",
    "                          'roberta_large_cnn_0.985_ep11', 'roberta_large_cnn_0.984_ep10']\n",
    "print('check_point_files_list:', check_point_files_list)\n",
    "\n",
    "df_test = pd.read_json(f'{DATASET_PATH}/val.json')\n",
    "\n",
    "for check_point_file in check_point_files_list:\n",
    "    print(f'\\n#######################################{check_point_file}')\n",
    "    check_point_file = f'{MODEL_SAVE_PATH}/{check_point_file}.pt'\n",
    "    embed_dim = 1024\n",
    "    model = ConvClassifier(embed_dim)\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(check_point_file))\n",
    "    evaluate(model, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
